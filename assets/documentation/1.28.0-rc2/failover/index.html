<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="The CloudNativePG Contributors" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Automated failover - CloudNativePG v1.28.0-rc2</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/override.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Automated failover";
        var mkdocs_page_input_path = "failover.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> CloudNativePG v1.28.0-rc2
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">CloudNativePG</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../before_you_start/">Before you start</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../use_cases/">Use cases</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../architecture/">Architecture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../installation_upgrade/">Installation and upgrades</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../quickstart/">Quickstart</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../image_catalog/">Image Catalog</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../bootstrap/">Bootstrap</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../database_import/">Importing Postgres databases</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../security/">Security</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../instance_manager/">Postgres Instance Manager</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../scheduling/">Scheduling</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../resource_management/">Resource management</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../failure_modes/">Failure Modes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../rolling_update/">Rolling updates</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../replication/">Replication</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../logical_replication/">Logical Replication</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../backup/">Backup</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../wal_archiving/">WAL archiving</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../recovery/">Recovery</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../service_management/">Service management</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../postgresql_conf/">PostgreSQL Configuration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../declarative_role_management/">PostgreSQL Role management</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../declarative_database_management/">PostgreSQL Database management</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../tablespaces/">Tablespaces</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../operator_conf/">Operator configuration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../cluster_conf/">Instance Pod configuration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../storage/">Storage</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../labels_annotations/">Labels and Annotations</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../monitoring/">Monitoring</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../logging/">Logging</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../certificates/">Certificates</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../ssl_connections/">Client TLS/SSL connections</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../applications/">Connecting from an application</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../connection_pooling/">Connection Pooling</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../replica_cluster/">Replica clusters</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../kubernetes_upgrade/">Kubernetes upgrade and maintenance</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../postgres_upgrades/">PostgreSQL upgrades</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../kubectl-plugin/">Kubectl Plugin</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Automated failover</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#rto-and-rpo-impact">RTO and RPO impact</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#delayed-failover">Delayed failover</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#failover-quorum-quorum-based-failover">Failover Quorum (Quorum-based Failover)</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#how-it-works">How it works</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#quorum-failover-example-scenarios">Quorum Failover Example Scenarios</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scenario-1-three-node-cluster-failing-pods">Scenario 1: Three-node cluster, failing pod(s)</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scenario-2-three-node-cluster-network-partition">Scenario 2: Three-node cluster, network partition</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scenario-3-five-node-cluster-network-partition">Scenario 3: Five-node cluster, network partition</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scenario-4-three-node-cluster-with-remote-synchronous-replicas">Scenario 4: Three-node cluster with remote synchronous replicas</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scenario-5-three-node-cluster-preferred-data-durability-network-partition">Scenario 5: Three-node cluster, preferred data durability, network partition</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../troubleshooting/">Troubleshooting</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../fencing/">Fencing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../declarative_hibernation/">Declarative hibernation</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../postgis/">PostGIS</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../e2e/">End-to-End Tests</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../container_images/">Container Image Requirements</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../imagevolume_extensions/">Image Volume Extensions</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../cnpg_i/">CNPG-I</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../operator_capability_levels/">Operator capability levels</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../controller/">Custom Pod Controller</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../samples/">Examples</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../networking/">Networking</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../benchmarking/">Benchmarking</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../faq/">Frequently Asked Questions (FAQ)</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../cloudnative-pg.v1/">API Reference</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../supported_releases/">Supported releases</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../preview_version/">Preview Versions</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../release_notes/">Release notes</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">CNCF Projects Integrations</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../cncf-projects/external-secrets/">External Secrets</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cncf-projects/cilium/">Cilium</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Appendixes</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../appendixes/backup_volumesnapshot/">Appendix A - Backup on volume snapshots</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../appendixes/backup_barmanobjectstore/">Appendix B - Backup on object stores</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../appendixes/object_stores/">Appendix C - Common object stores for backups</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">CloudNativePG v1.28.0-rc2</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Automated failover</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="automated-failover">Automated failover</h1>
<!-- SPDX-License-Identifier: CC-BY-4.0 -->

<p>In the case of unexpected errors on the primary for longer than the
<code>.spec.failoverDelay</code> (by default <code>0</code> seconds), the cluster will go into
<strong>failover mode</strong>. This may happen, for example, when:</p>
<ul>
<li>The primary pod has a disk failure</li>
<li>The primary pod is deleted</li>
<li>The <code>postgres</code> container on the primary has any kind of sustained failure</li>
</ul>
<p>In the failover scenario, the primary cannot be assumed to be working properly.</p>
<p>After cases like the ones above, the readiness probe for the primary pod will start
failing. This will be picked up in the controller's reconciliation loop. The
controller will initiate the failover process, in two steps:</p>
<ol>
<li>First, it will mark the <code>TargetPrimary</code> as <code>pending</code>. This change of state will
   force the primary pod to shutdown, to ensure the WAL receivers on the replicas
   will stop. The cluster will be marked in failover phase ("Failing over").</li>
<li>Once all WAL receivers are stopped, there will be a leader election, and a
   new primary will be named. The chosen instance will initiate promotion to
   primary, and, after this is completed, the cluster will resume normal operations.
   Meanwhile, the former primary pod will restart, detect that it is no longer
   the primary, and become a replica node.</li>
</ol>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The two-phase procedure helps ensure the WAL receivers can stop in an orderly
fashion, and that the failing primary will not start streaming WALs again upon
restart. These safeguards prevent timeline discrepancies between the new primary
and the replicas.</p>
</div>
<p>During the time the failing primary is being shut down:</p>
<ol>
<li>It will first try a PostgreSQL's <em>fast shutdown</em> with
   <code>.spec.switchoverDelay</code> seconds as timeout. This graceful shutdown will attempt
   to archive pending WALs.</li>
<li>If the fast shutdown fails, or its timeout is exceeded, a PostgreSQL's
   <em>immediate shutdown</em> is initiated.</li>
</ol>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>"Fast" mode does not wait for PostgreSQL clients to disconnect and will
terminate an online backup in progress. All active transactions are rolled back
and clients are forcibly disconnected, then the server is shut down.
"Immediate" mode will abort all PostgreSQL server processes immediately,
without a clean shutdown.</p>
</div>
<h2 id="rto-and-rpo-impact">RTO and RPO impact</h2>
<p>Failover may result in the service being impacted (<a href="../before_you_start/#rto">RTO</a>)
and/or data being lost (<a href="../before_you_start/#rpo">RPO</a>):</p>
<ol>
<li>During the time when the primary has started to fail, and before the controller
   starts failover procedures, queries in transit, WAL writes, checkpoints and
   similar operations, may fail.</li>
<li>Once the fast shutdown command has been issued, the cluster will no longer
   accept connections, so service will be impacted but no data
   will be lost.</li>
<li>If the fast shutdown fails, the immediate shutdown will stop any pending
   processes, including WAL writing. Data may be lost.</li>
<li>During the time the primary is shutting down and a new primary hasn't yet
   started, the cluster will operate without a primary and thus be impaired - but
   with no data loss.</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The timeout that controls fast shutdown is set by <code>.spec.switchoverDelay</code>,
as in the case of a switchover. Increasing the time for fast shutdown is safer
from an RPO point of view, but possibly delays the return to normal operation -
negatively affecting RTO.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>As already mentioned in the <a href="../instance_manager/">"Instance Manager" section</a>
when explaining the switchover process, the <code>.spec.switchoverDelay</code> option
affects the RPO and RTO of your PostgreSQL database. Setting it to a low value,
might favor RTO over RPO but lead to data loss at cluster level and/or backup
level. On the contrary, setting it to a high value, might remove the risk of
data loss while leaving the cluster without an active primary for a longer time
during the switchover.</p>
</div>
<h2 id="delayed-failover">Delayed failover</h2>
<p>As anticipated above, the <code>.spec.failoverDelay</code> option allows you to delay the start
of the failover procedure by a number of seconds after the primary has been
detected to be unhealthy. By default, this setting is set to <code>0</code>, triggering the
failover procedure immediately.</p>
<p>Sometimes failing over to a new primary can be more disruptive than waiting
for the primary to come back online. This is especially true of network
disruptions where multiple tiers are affected (i.e., downstream logical
subscribers) or when the time to perform the failover is longer than the
expected outage.</p>
<p>Enabling a new configuration option to delay failover provides a mechanism to
prevent premature failover for short-lived network or node instability.</p>
<h2 id="failover-quorum-quorum-based-failover">Failover Quorum (Quorum-based Failover)</h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><em>Failover quorum</em> is an experimental feature introduced in version 1.27.0.
Use with caution in production environments.</p>
</div>
<p>Failover quorum is a mechanism that enhances data durability and safety during
failover events in CloudNativePG-managed PostgreSQL clusters.</p>
<p>Quorum-based failover allows the controller to determine whether to promote a replica
to primary based on the state of a quorum of replicas.
This is useful when stronger data durability is required than the one offered
by <a href="../replication/#synchronous-replication">synchronous replication</a> and
default automated failover procedures.</p>
<p>When synchronous replication is not enabled, some data loss is expected and
accepted during failover, as a replica may lag behind the primary when
promoted.</p>
<p>With synchronous replication enabled, the guarantee is that the application
will not receive explicit acknowledgment of the successful commit of a
transaction until the WAL data is known to be safely received by all required
synchronous standbys.
This is not enough to guarantee that the operator is able to promote the most
advanced replica.</p>
<p>For example, in a three-node cluster with synchronous replication set to <code>ANY 1
(...)</code>, data is written to the primary and one standby before a commit is
acknowledged. If both the primary and the aligned standby become unavailable
(such as during a network partition), the remaining replica may not have the
latest data. Promoting it could lose some data that the application considered
committed.</p>
<p>Quorum-based failover addresses this risk by ensuring that failover only occurs
if the operator can confirm the presence of all synchronously committed data in
the instance to promote, and it does not occur otherwise.</p>
<p>This feature allows users to choose their preferred trade-off between data
durability and data availability.</p>
<p>Failover quorum can be enabled by setting the
<code>.spec.postgresql.synchronous.failoverQuorum</code> field to <code>true</code>:</p>
<pre><code class="language-yaml">apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: cluster-example
spec:
  instances: 3

  postgresql:
    synchronous:
      method: any
      number: 1
      failoverQuorum: true

  storage:
    size: 1Gi
</code></pre>
<p>For backward compatibility, the legacy annotation
<code>alpha.cnpg.io/failoverQuorum</code> is still supported by the admission webhook and
takes precedence over the <code>Cluster</code> spec option:</p>
<ul>
<li>If the annotation evaluates to <code>"true"</code> and a synchronous replication stanza
  is present, the webhook automatically sets
  <code>.spec.postgresql.synchronous.failoverQuorum</code> to <code>true</code>.</li>
<li>If the annotation evaluates to <code>"false"</code>, the feature is always disabled</li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Because the annotation overrides the spec, we recommend that users of this
experimental feature migrate to the native
<code>.spec.postgresql.synchronous.failoverQuorum</code> option and remove the annotation
from their manifests. The annotation is <strong>deprecated</strong> and will be removed in a
future release.</p>
</div>
<h3 id="how-it-works">How it works</h3>
<p>Before promoting a replica to primary, the operator performs a quorum check,
following the principles of the Dynamo <code>R + W &gt; N</code> consistency model<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>.</p>
<p>In the quorum failover, these values assume the following meaning:</p>
<ul>
<li><code>R</code> is the number of <em>promotable replicas</em> (read quorum);</li>
<li><code>W</code> is the number of replicas that must acknowledge the write before the
  <code>COMMIT</code> is returned to the client (write quorum);</li>
<li><code>N</code> is the total number of potentially synchronous replicas;</li>
</ul>
<p><em>Promotable replicas</em> are replicas that have these properties:</p>
<ul>
<li>are part of the cluster;</li>
<li>are able to report their state to the operator;</li>
<li>are potentially synchronous;</li>
</ul>
<p>If <code>R + W &gt; N</code>, then we can be sure that among the promotable replicas there is
at least one that has confirmed all the synchronous commits, and we can safely
promote it to primary. If this is not the case, the controller will not promote
any replica to primary, and will wait for the situation to change.</p>
<p>Users can force a promotion of a replica to primary through the
<code>kubectl cnpg promote</code> command even if the quorum check is failing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Manual promotion should only be used as a last resort. Before proceeding,
make sure you fully understand the risk of data loss and carefully consider the
consequences of prioritizing the resumption of write workloads for your
applications.</p>
</div>
<p>An additional CRD is used to track the quorum state of the cluster. A <code>Cluster</code>
with the quorum failover enabled will have a <code>FailoverQuorum</code> resource with the same
name as the <code>Cluster</code> resource. The <code>FailoverQuorum</code> CR is created by the
controller when the quorum failover is enabled, and it is updated by the primary
instance during its reconciliation loop, and read by the operator during quorum
checks. It is used to track the latest known configuration of the synchronous
replication.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Users should not modify the <code>FailoverQuorum</code> resource directly. During
PostgreSQL configuration changes, when it is not possible to determine the
configuration, the <code>FailoverQuorum</code> resource will be reset, preventing any
failover until the new configuration is applied.</p>
</div>
<p>The <code>FailoverQuorum</code> resource works in conjunction with PostgreSQL synchronous
replication.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>There is no guarantee that <code>COMMIT</code> operations returned to the
client but that have not been performed synchronously, such as those made
explicitly disabling synchronous replication with
<code>SET synchronous_commit TO local</code>, will be present on a promoted replica.</p>
</div>
<h3 id="quorum-failover-example-scenarios">Quorum Failover Example Scenarios</h3>
<p>In the following scenarios, <code>R</code> is the number of promotable replicas, <code>W</code> is
the number of replicas that must acknowledge a write before commit, and <code>N</code> is
the total number of potentially synchronous replicas. The "Failover" column
indicates whether failover is allowed under quorum failover rules.</p>
<h4 id="scenario-1-three-node-cluster-failing-pods">Scenario 1: Three-node cluster, failing pod(s)</h4>
<p>A cluster with <code>instances: 3</code>, <code>synchronous.number=1</code>, and
<code>dataDurability=required</code>.</p>
<ul>
<li>If only the primary fails, two promotable replicas remain (R=2).
  Since <code>R + W &gt; N</code> (2 + 1 &gt; 2), failover is allowed and safe.</li>
<li>If both the primary and one replica fail, only one promotable replica
  remains (R=1). Since <code>R + W = N</code> (1 + 1 = 2), failover is not allowed to
  prevent possible data loss.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">R</th>
<th style="text-align: center;">W</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">Failover</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">✅</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">❌</td>
</tr>
</tbody>
</table>
<h4 id="scenario-2-three-node-cluster-network-partition">Scenario 2: Three-node cluster, network partition</h4>
<p>A cluster with <code>instances: 3</code>, <code>synchronous.number: 1</code>, and
<code>dataDurability: required</code> experiences a network partition.</p>
<ul>
<li>If the operator can communicate with the primary, no failover occurs. The
  cluster can be impacted if the primary cannot reach any standby, since it
  won't commit transactions due to synchronous replication requirements.</li>
<li>If the operator cannot reach the primary but can reach both replicas (R=2),
  failover is allowed. If the operator can reach only one replica (R=1),
  failover is not allowed, as the synchronous one may be the other one.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">R</th>
<th style="text-align: center;">W</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">Failover</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">✅</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">❌</td>
</tr>
</tbody>
</table>
<h4 id="scenario-3-five-node-cluster-network-partition">Scenario 3: Five-node cluster, network partition</h4>
<p>A cluster with <code>instances: 5</code>, <code>synchronous.number=2</code>, and
<code>dataDurability=required</code> experiences a network partition.</p>
<ul>
<li>If the operator can communicate with the primary, no failover occurs. The
  cluster can be impacted if the primary cannot reach at least two standbys,
  as since it won't commit transactions due to synchronous replication
  requirements.</li>
<li>If the operator cannot reach the primary but can reach at least three
  replicas (R=3), failover is allowed. If the operator can reach only two
  replicas (R=2), failover is not allowed, as the synchronous one may be the
  other one.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">R</th>
<th style="text-align: center;">W</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">Failover</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">✅</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">❌</td>
</tr>
</tbody>
</table>
<h4 id="scenario-4-three-node-cluster-with-remote-synchronous-replicas">Scenario 4: Three-node cluster with remote synchronous replicas</h4>
<p>A cluster with <code>instances: 3</code> and remote synchronous replicas defined in
<code>standbyNamesPre</code> or <code>standbyNamesPost</code>. We assume that the primary is failing.</p>
<p>This scenario requires an important consideration. Replicas listed in
<code>standbyNamesPre</code> or <code>standbyNamesPost</code> are not counted in
<code>R</code> (they cannot be promoted), but are included in <code>N</code> (they may have received
synchronous writes). So, if
<code>synchronous.number &lt;= len(standbyNamesPre) + len(standbyNamesPost)</code>, failover
is not possible, as no local replica can be guaranteed to have the required
data. The operator prevents such configurations during validation, but some
invalid configurations are shown below for clarity.</p>
<p><strong>Example configurations:</strong></p>
<p>Configuration #1 (valid):</p>
<pre><code class="language-yaml">instances: 3
postgresql:
  synchronous:
    method: any
    number: 2
    standbyNamesPre:
      - angus
</code></pre>
<p>In this configuration, when the primary fails, <code>R = 2</code> (the local replicas),
<code>W = 2</code>, and <code>N = 3</code> (2 local replicas + 1 remote), allowing failover.
In case of an additional replica failing (<code>R = 1</code>) failover is not allowed.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">R</th>
<th style="text-align: center;">W</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">Failover</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">✅</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">❌</td>
</tr>
</tbody>
</table>
<p>Configuration #2 (invalid):</p>
<pre><code class="language-yaml">instances: 3
postgresql:
  synchronous:
    method: any
    number: 1
    maxStandbyNamesFromCluster: 1
    standbyNamesPre:
      - angus
</code></pre>
<p>In this configuration, <code>R = 2</code> (the local replicas), <code>W = 1</code>, and <code>N = 3</code>
(2 local replicas + 1 remote).
Failover is not possible in this setup, so quorum failover can not be
enabled with this configuration.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">R</th>
<th style="text-align: center;">W</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">Failover</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">❌</td>
</tr>
</tbody>
</table>
<p>Configuration #3 (invalid):</p>
<pre><code class="language-yaml">instances: 3
postgresql:
  synchronous:
    method: any
    number: 1
    maxStandbyNamesFromCluster: 0
    standbyNamesPre:
      - angus
      - malcolm
</code></pre>
<p>In this configuration, <code>R = 0</code> (the local replicas), <code>W = 1</code>, and <code>N = 2</code>
(0 local replicas + 2 remote).
Failover is not possible in this setup, so quorum failover can not be
enabled with this configuration.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">R</th>
<th style="text-align: center;">W</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">Failover</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">❌</td>
</tr>
</tbody>
</table>
<h4 id="scenario-5-three-node-cluster-preferred-data-durability-network-partition">Scenario 5: Three-node cluster, preferred data durability, network partition</h4>
<p>Consider a cluster with <code>instances: 3</code>, <code>synchronous.number=1</code>, and
<code>dataDurability=preferred</code> that experiences a network partition.</p>
<ul>
<li>If the operator can communicate with both the primary and the API server,
  the primary continues to operate, removing unreachable standbys from the
  <code>synchronous_standby_names</code> set.</li>
<li>If the primary cannot reach the operator or API server, a quorum check is
  performed. The <code>FailoverQuorum</code> status cannot have changed, as the primary cannot
  have received new configuration. If the operator can reach both replicas,
  failover is allowed (<code>R=2</code>). If only one replica is reachable (<code>R=1</code>),
  failover is not allowed.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">R</th>
<th style="text-align: center;">W</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">Failover</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">✅</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">❌</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p><a href="https://www.amazon.science/publications/dynamo-amazons-highly-available-key-value-store">Dynamo: Amazon’s highly available key-value store</a>&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../kubectl-plugin/" class="btn btn-neutral float-left" title="Kubectl Plugin"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../troubleshooting/" class="btn btn-neutral float-right" title="Troubleshooting">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
      <p>Copyright © CloudNativePG a Series of LF Projects, LLC</p>
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../kubectl-plugin/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../troubleshooting/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
