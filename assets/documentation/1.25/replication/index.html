<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="The CloudNativePG Contributors" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Replication - CloudNativePG v1.25</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/override.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Replication";
        var mkdocs_page_input_path = "replication.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> CloudNativePG v1.25
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">CloudNativePG</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../before_you_start/">Before You Start</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../use_cases/">Use cases</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../architecture/">Architecture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../installation_upgrade/">Installation and upgrades</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../quickstart/">Quickstart</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../image_catalog/">Image Catalog</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../bootstrap/">Bootstrap</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../database_import/">Importing Postgres databases</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../security/">Security</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../instance_manager/">Postgres instance manager</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../scheduling/">Scheduling</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../resource_management/">Resource management</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../failure_modes/">Failure Modes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../rolling_update/">Rolling Updates</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Replication</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#application-level-replication">Application-level replication</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#a-very-mature-technology">A very mature technology</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#streaming-replication-support">Streaming replication support</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#continuous-backup-integration">Continuous backup integration</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#synchronous-replication">Synchronous Replication</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#quorum-based-synchronous-replication">Quorum-based Synchronous Replication</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#example">Example</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#migrating-from-deprecated-synchronous-replication-implementation">Migrating from Deprecated Synchronous Replication Implementation</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#priority-based-synchronous-replication">Priority-based Synchronous Replication</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#controlling-synchronous_standby_names-content">Controlling synchronous_standby_names Content</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#examples">Examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#data-durability-and-synchronous-replication">Data Durability and Synchronous Replication</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#required-data-durability">Required Data Durability</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#preferred-data-durability">Preferred Data Durability</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#synchronous-replication-deprecated">Synchronous Replication (Deprecated)</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#select-nodes-for-synchronous-replication">Select nodes for synchronous replication</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#replication-slots">Replication slots</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#replication-slots-for-high-availability">Replication slots for High Availability</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#user-defined-replication-slots">User-Defined Replication slots</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#synchronization-frequency">Synchronization frequency</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#capping-the-wal-size-retained-for-replication-slots">Capping the WAL size retained for replication slots</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#monitoring-replication-slots">Monitoring replication slots</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../logical_replication/">Logical Replication</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../backup/">Backup</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../backup_barmanobjectstore/">Backup on object stores</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../wal_archiving/">WAL archiving</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../backup_volumesnapshot/">Backup on volume snapshots</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../recovery/">Recovery</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../service_management/">Service Management</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../postgresql_conf/">PostgreSQL Configuration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../declarative_role_management/">PostgreSQL Role Management</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../declarative_database_management/">PostgreSQL Database Management</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../tablespaces/">Tablespaces</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../operator_conf/">Operator configuration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../cluster_conf/">Instance pod configuration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../storage/">Storage</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../labels_annotations/">Labels and annotations</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../monitoring/">Monitoring</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../logging/">Logging</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../certificates/">Certificates</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../ssl_connections/">Client TLS/SSL connections</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../applications/">Connecting from an application</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../connection_pooling/">Connection pooling</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../replica_cluster/">Replica clusters</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../kubernetes_upgrade/">Kubernetes Upgrade and Maintenance</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../kubectl-plugin/">Kubectl Plugin</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../failover/">Automated failover</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../troubleshooting/">Troubleshooting</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../fencing/">Fencing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../declarative_hibernation/">Declarative hibernation</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../postgis/">PostGIS</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../e2e/">End-to-End Tests</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../container_images/">Container Image Requirements</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../operator_capability_levels/">Operator capability levels</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../controller/">Custom Pod Controller</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../samples/">Examples</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../networking/">Networking</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../benchmarking/">Benchmarking</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../faq/">Frequently Asked Questions (FAQ)</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../cloudnative-pg.v1/">API Reference</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../supported_releases/">Supported releases</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../preview_version/">Preview Versions</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../release_notes/">Release notes</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Appendixes</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../appendixes/object_stores/">Appendix A - Common object stores for backups</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">CloudNativePG v1.25</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Replication</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="replication">Replication</h1>
<p>Physical replication is one of the strengths of PostgreSQL and one of the
reasons why some of the largest organizations in the world have chosen it for
the management of their data in business continuity contexts. Primarily used to
achieve high availability, physical replication also allows scale-out of
read-only workloads and offloading of some work from the primary.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>This section is about replication within the same <code>Cluster</code> resource
managed in the same Kubernetes cluster. For information about how to
replicate with another Postgres <code>Cluster</code> resource, even across different
Kubernetes clusters, please refer to the
<a href="../replica_cluster/">"Replica clusters"</a> section.</p>
</div>
<h2 id="application-level-replication">Application-level replication</h2>
<p>Having contributed throughout the years to the replication feature in
PostgreSQL, we have decided to build high availability in CloudNativePG on top
of the native physical replication technology, and integrate it directly in the
Kubernetes API.</p>
<p>In Kubernetes terms, this is referred to as <strong>application-level replication</strong>,
in contrast with <em>storage-level replication</em>.</p>
<h2 id="a-very-mature-technology">A very mature technology</h2>
<p>PostgreSQL has a very robust and mature native framework for replicating data
from the primary instance to one or more replicas, built around the concept of
transactional changes continuously stored in the WAL (Write Ahead Log).</p>
<p>Started as the evolution of crash recovery and point in time recovery
technologies, physical replication was first introduced in PostgreSQL 8.2
(2006) through WAL shipping from the primary to a warm standby in continuous
recovery.</p>
<p>PostgreSQL 9.0 (2010) introduced WAL streaming and read-only replicas through
<em>hot standby</em>. In 2011, PostgreSQL 9.1 brought synchronous replication at the
transaction level, supporting <a href="../before_you_start/#rpo">RPO</a>=0 clusters. Cascading
replication was added in PostgreSQL 9.2 (2012). The foundations for
<a href="../logical_replication/">logical replication</a> were established in PostgreSQL
9.4 (2014), and version 10 (2017) introduced native support for the
publisher/subscriber pattern to replicate data from an origin to a destination. The
table below summarizes these milestones.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Version</th>
<th style="text-align: center;">Year</th>
<th>Feature</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">2006</td>
<td>Warm Standby with WAL shipping</td>
</tr>
<tr>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">2010</td>
<td>Hot Standby and physical streaming replication</td>
</tr>
<tr>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">2011</td>
<td>Synchronous replication (priority-based)</td>
</tr>
<tr>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">2012</td>
<td>Cascading replication</td>
</tr>
<tr>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">2014</td>
<td>Foundations of logical replication</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2017</td>
<td>Logical publisher/subscriber and quorum-based synchronous replication</td>
</tr>
</tbody>
</table>
<p>This table highlights key PostgreSQL replication features and their respective
versions.</p>
<h2 id="streaming-replication-support">Streaming replication support</h2>
<p>At the moment, CloudNativePG natively and transparently manages physical
streaming replicas within a cluster in a declarative way, based on the number of
provided <code>instances</code> in the <code>spec</code>:</p>
<pre><code>replicas = instances - 1 (where  instances &gt; 0)
</code></pre>
<p>Immediately after the initialization of a cluster, the operator creates a user
called <code>streaming_replica</code> as follows:</p>
<pre><code class="language-sql">CREATE USER streaming_replica WITH REPLICATION;
-- NOSUPERUSER INHERIT NOCREATEROLE NOCREATEDB NOBYPASSRLS
</code></pre>
<p>Out of the box, the operator automatically sets up streaming replication within
the cluster over an encrypted channel and enforces TLS client certificate
authentication for the <code>streaming_replica</code> user - as highlighted by the
following excerpt taken from <code>pg_hba.conf</code>:</p>
<pre><code># Require client certificate authentication for the streaming_replica user
hostssl postgres streaming_replica all cert
hostssl replication streaming_replica all cert
</code></pre>
<div class="admonition seealso">
<p class="admonition-title">Certificates</p>
<p>For details on how CloudNativePG manages certificates, please refer
to the <a href="../certificates/#client-streaming_replica-certificate">"Certificates" section</a>
in the documentation.</p>
</div>
<p>If configured, the operator manages replication slots for all the replicas in the
HA cluster, ensuring that WAL files required by each standby are retained on
the primary's storage, even after a failover or switchover.</p>
<div class="admonition seealso">
<p class="admonition-title">Replication slots for High Availability</p>
<p>For details on how CloudNativePG automatically manages replication slots for the
High Availability replicas, please refer to the
<a href="#replication-slots-for-high-availability">"Replication slots for High Availability" section</a>
below.</p>
</div>
<h3 id="continuous-backup-integration">Continuous backup integration</h3>
<p>In case continuous backup is configured in the cluster, CloudNativePG
transparently configures replicas to take advantage of <code>restore_command</code> when in
continuous recovery. As a result, PostgreSQL can use the WAL archive as a
fallback option whenever pulling WALs via streaming replication fails.</p>
<h2 id="synchronous-replication">Synchronous Replication</h2>
<p>CloudNativePG supports both
<a href="https://www.postgresql.org/docs/current/warm-standby.html#SYNCHRONOUS-REPLICATION">quorum-based and priority-based synchronous replication for PostgreSQL</a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>By default, synchronous replication pauses write operations if the required
number of standby nodes for WAL replication during transaction commits is
unavailable. This behavior prioritizes data durability and aligns with
PostgreSQL DBA best practices. However, if self-healing is a higher priority
than strict data durability in your setup, this setting can be adjusted. For
details on managing this behavior, refer to the <a href="#data-durability-and-synchronous-replication">Data Durability and Synchronous Replication</a>
section.</p>
</div>
<p>Direct configuration of the <code>synchronous_standby_names</code> option is not
permitted. However, CloudNativePG automatically populates this option with the
names of local pods, while also allowing customization to extend synchronous
replication beyond the <code>Cluster</code> resource.
This can be achieved through the
<a href="../cloudnative-pg.v1/#postgresql-cnpg-io-v1-SynchronousReplicaConfiguration"><code>.spec.postgresql.synchronous</code> stanza</a>.</p>
<p>Synchronous replication is disabled by default (the <code>synchronous</code> stanza is not
defined). When defined, two options are mandatory:</p>
<ul>
<li><code>method</code>: either <code>any</code> (quorum) or <code>first</code> (priority)</li>
<li><code>number</code>: the number of synchronous standby servers that transactions must
  wait for responses from</li>
</ul>
<h3 id="quorum-based-synchronous-replication">Quorum-based Synchronous Replication</h3>
<p>In PostgreSQL, quorum-based synchronous replication ensures that transaction
commits wait until their WAL records are replicated to a specified number of
standbys. To enable this, set the <code>method</code> to <code>any</code>.</p>
<p>This replication method is the most common setup for a CloudNativePG cluster.</p>
<h4 id="example">Example</h4>
<p>The example below, based on a typical <code>cluster-example</code> configuration with
three instances, sets up quorum-based synchronous replication with at least one
instance:</p>
<pre><code class="language-yaml">postgresql:
  synchronous:
    method: any
    number: 1
</code></pre>
<p>With this configuration, CloudNativePG automatically sets the content of
<code>synchronous_standby_names</code> as follows:</p>
<pre><code class="language-console">ANY 1 (cluster-example-2, cluster-example-3, cluster-example-1)
</code></pre>
<h4 id="migrating-from-deprecated-synchronous-replication-implementation">Migrating from Deprecated Synchronous Replication Implementation</h4>
<p>This section outlines how to migrate from the deprecated quorum-based
synchronous replication format to the newer, more robust implementation in
CloudNativePG.</p>
<p>Given the following manifest:</p>
<pre><code class="language-yaml">apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: angus
spec:
  instances: 3
  minSyncReplicas: 1
  maxSyncReplicas: 1

  storage:
    size: 1G
</code></pre>
<p>You can update it to the new format as follows:</p>
<pre><code class="language-yaml">apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: angus
spec:
  instances: 3

  storage:
    size: 1G

  postgresql:
    synchronous:
      method: any
      number: 1
      dataDurability: required
</code></pre>
<p>To prioritize self-healing over strict data durability, set <code>dataDurability</code>
to <code>preferred</code> instead.</p>
<h3 id="priority-based-synchronous-replication">Priority-based Synchronous Replication</h3>
<p>PostgreSQL's priority-based synchronous replication makes transaction commits
wait until their WAL records are replicated to the requested number of
synchronous standbys chosen based on their priorities. Standbys listed earlier
in the <code>synchronous_standby_names</code> option are given higher priority and
considered synchronous. If a current synchronous standby disconnects, it is
immediately replaced by the next-highest-priority standby. To use this method,
set <code>method</code> to <code>first</code>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Currently, this method is most useful when extending
synchronous replication beyond the current cluster using the
<code>maxStandbyNamesFromCluster</code>, <code>standbyNamesPre</code>, and <code>standbyNamesPost</code>
options explained below.</p>
</div>
<h3 id="controlling-synchronous_standby_names-content">Controlling <code>synchronous_standby_names</code> Content</h3>
<p>By default, CloudNativePG populates <code>synchronous_standby_names</code> with the names
of local pods in a <code>Cluster</code> resource, ensuring synchronous replication within
the PostgreSQL cluster. You can customize the content of
<code>synchronous_standby_names</code> based on your requirements and replication method
(quorum or priority) using the following optional parameters in the
<code>.spec.postgresql.synchronous</code> stanza:</p>
<ul>
<li><code>maxStandbyNamesFromCluster</code>: the maximum number of pod names from the local
  <code>Cluster</code> object that can be automatically included in the
  <code>synchronous_standby_names</code> option in PostgreSQL.</li>
<li><code>standbyNamesPre</code>: a list of standby names (specifically <code>application_name</code>)
  to be prepended to the list of local pod names automatically listed by the
  operator.</li>
<li><code>standbyNamesPost</code>: a list of standby names (specifically <code>application_name</code>)
  to be appended to the list of local pod names automatically listed by the
  operator.</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You are responsible for ensuring the correct names in <code>standbyNamesPre</code> and
<code>standbyNamesPost</code>. CloudNativePG expects that you manage any standby with
an <code>application_name</code> listed here, ensuring their high availability.
Incorrect entries can jeopardize your PostgreSQL database uptime.</p>
</div>
<h4 id="examples">Examples</h4>
<p>Here are some examples, all based on a <code>cluster-example</code> with three instances:</p>
<p>If you set:</p>
<pre><code class="language-yaml">postgresql:
  synchronous:
    method: any
    number: 1
    maxStandbyNamesFromCluster: 1
    standbyNamesPre:
      - angus
</code></pre>
<p>The content of <code>synchronous_standby_names</code> will be:</p>
<pre><code class="language-console">ANY 1 (angus, cluster-example-2)
</code></pre>
<p>If you set:</p>
<pre><code class="language-yaml">postgresql:
  synchronous:
    method: any
    number: 1
    maxStandbyNamesFromCluster: 0
    standbyNamesPre:
      - angus
      - malcolm
</code></pre>
<p>The content of <code>synchronous_standby_names</code> will be:</p>
<pre><code class="language-console">ANY 1 (angus, malcolm)
</code></pre>
<p>If you set:</p>
<pre><code class="language-yaml">postgresql:
  synchronous:
    method: first
    number: 2
    maxStandbyNamesFromCluster: 1
    standbyNamesPre:
      - angus
    standbyNamesPost:
      - malcolm
</code></pre>
<p>The <code>synchronous_standby_names</code> option will look like:</p>
<pre><code class="language-console">FIRST 2 (angus, cluster-example-2, malcolm)
</code></pre>
<h3 id="data-durability-and-synchronous-replication">Data Durability and Synchronous Replication</h3>
<p>The <code>dataDurability</code> option in the <code>.spec.postgresql.synchronous</code> stanza
controls the trade-off between data safety and availability for synchronous
replication. It can be set to <code>required</code> or <code>preferred</code>, with the default being
<code>required</code> if not specified.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><code>preferred</code> can only be used when <code>standbyNamesPre</code> and <code>standbyNamesPost</code>
are unset.</p>
</div>
<h4 id="required-data-durability">Required Data Durability</h4>
<p>When <code>dataDurability</code> is set to <code>required</code>, PostgreSQL only considers
transactions committed once WAL (Write-Ahead Log) records have been replicated
to the specified number of synchronous standbys. This setting prioritizes data
safety over availability, meaning write operations will pause if the required
number of synchronous standbys is unavailable. This ensures zero data loss
(RPO=0) but may reduce database availability during network disruptions or
standby failures.</p>
<p>Synchronous standbys are selected in this priority order:</p>
<ol>
<li>Healthy instances</li>
<li>Unhealthy instances</li>
<li>Primary</li>
</ol>
<p>The list is then truncated based on <code>maxStandbyNamesFromCluster</code> if this value
is set, prioritizing healthy instances and ensuring <code>synchronous_standby_names</code>
is populated.</p>
<h5 id="example_1">Example</h5>
<p>Consider the following example:</p>
<pre><code class="language-yaml">apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: foo
spec:
  instances: 3
  postgresql:
    synchronous:
      method: any
      number: 1
      dataDurability: required
</code></pre>
<ol>
<li>
<p>Initial state. The content of <code>synchronous_standby_names</code> is:</p>
<p><code>ANY 1 ("foo-2","foo-3","foo-1")</code></p>
</li>
<li>
<p><code>foo-2</code> becomes unavailable. It gets pushed back in priority:</p>
<p><code>ANY 1 ("foo-3","foo-2","foo-1")</code></p>
</li>
<li>
<p><code>foo-3</code> also becomes unavailable. The list contains no healthy standbys:</p>
<p><code>ANY 1 ("foo-2","foo-3","foo-1")</code></p>
<p>At this point no write operations will be allowed until at least one of the
standbys is available again.</p>
</li>
<li>
<p>When the standbys are available again, <code>synchronous_standby_names</code> will
   be back to the initial state.</p>
</li>
</ol>
<h4 id="preferred-data-durability">Preferred Data Durability</h4>
<p>When <code>dataDurability</code> is set to <code>preferred</code>, the required number of synchronous
instances adjusts based on the number of available standbys. PostgreSQL will
attempt to replicate WAL records to the designated number of synchronous
standbys, but write operations will continue even if fewer than the requested
number of standbys are available.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Make sure you have a clear understanding of what <em>ready/available</em> means
for a replica and set your expectations accordingly. By default, a replica is
considered ready when it has successfully connected to the source at least
once.</p>
</div>
<p>This setting balances data safety with availability, enabling applications to
continue writing during temporary standby unavailability—hence, it’s also known
as <em>self-healing mode</em>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This mode may result in data loss if all standbys become unavailable.</p>
</div>
<p>With <code>preferred</code> data durability, <strong>only healthy replicas</strong> are included in
<code>synchronous_standby_names</code>.</p>
<h5 id="example_2">Example</h5>
<p>Consider the following example. For demonstration, we’ll use a cluster named
<code>bar</code> with 5 instances and 2 synchronous standbys:</p>
<pre><code class="language-yaml">apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: bar
spec:
  instances: 5
  postgresql:
    synchronous:
      method: any
      number: 2
      dataDurability: preferred
</code></pre>
<ol>
<li>
<p>Initial state. The content of <code>synchronous_standby_names</code> is:</p>
<p><code>ANY 2 ("bar-2","bar-3", "bar-4", "bar-5")</code></p>
</li>
<li>
<p><code>bar-2</code> and <code>bar-3</code> become unavailable. They are removed from the list:</p>
<p><code>ANY 2 ("bar-4", "bar-5")</code></p>
</li>
<li>
<p><code>bar-4</code> also becomes unavailable. It gets removed from the list. Since the
   number of available standbys is less than the requested number, the requested
   amount gets reduced:</p>
<p><code>ANY 1 ("bar-5")</code></p>
</li>
<li>
<p><code>bar-5</code> also becomes unavailable. <code>synchronous_standby_names</code> becomes empty,
   disabling synchronous replication completely. Write operations will continue,
   but with the risk of potential data loss in case of a primary failure.</p>
</li>
<li>When the replicas are back, <code>synchronous_standby_names</code> will be back to
   the initial state.</li>
</ol>
<h2 id="synchronous-replication-deprecated">Synchronous Replication (Deprecated)</h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Prior to CloudNativePG 1.24, only the quorum-based synchronous replication
implementation was supported. Although this method is now deprecated, it
will not be removed anytime soon.
The new method prioritizes data durability over self-healing and offers more
robust features, including priority-based synchronous replication and full
control over the <code>synchronous_standby_names</code> option.
It is recommended to gradually migrate to the new configuration method for
synchronous replication, as explained in the previous paragraph.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The deprecated method and the new method are mutually exclusive.</p>
</div>
<p>CloudNativePG supports the configuration of <strong>quorum-based synchronous
streaming replication</strong> via two configuration options called <code>minSyncReplicas</code>
and <code>maxSyncReplicas</code>, which are the minimum and the maximum number of expected
synchronous standby replicas available at any time.
For self-healing purposes, the operator always compares these two values with
the number of available replicas to determine the quorum.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>By default, synchronous replication selects among all the available
replicas indistinctively. You can limit on which nodes your synchronous
replicas can be scheduled, by working on node labels through the
<code>syncReplicaElectionConstraint</code> option as described in the next section.</p>
</div>
<p>Synchronous replication is disabled by default (<code>minSyncReplicas</code> and
<code>maxSyncReplicas</code> are not defined).
In case both <code>minSyncReplicas</code> and <code>maxSyncReplicas</code> are set, CloudNativePG
automatically updates the <code>synchronous_standby_names</code> option in
PostgreSQL to the following value:</p>
<pre><code>ANY q (pod1, pod2, ...)
</code></pre>
<p>Where:</p>
<ul>
<li><code>q</code> is an integer automatically calculated by the operator to be:
  <code>1 &lt;= minSyncReplicas &lt;= q &lt;= maxSyncReplicas &lt;= readyReplicas</code></li>
<li><code>pod1, pod2, ...</code> is the list of all PostgreSQL pods in the cluster</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>To provide self-healing capabilities, the operator can ignore
<code>minSyncReplicas</code> if such value is higher than the currently available
number of replicas. Synchronous replication is automatically disabled
when <code>readyReplicas</code> is <code>0</code>.</p>
</div>
<p>As stated in the
<a href="https://www.postgresql.org/docs/current/warm-standby.html#SYNCHRONOUS-REPLICATION">PostgreSQL documentation</a>,
the <em>method <code>ANY</code> specifies a quorum-based synchronous replication and makes
transaction commits wait until their WAL records are replicated to at least the
requested number of synchronous standbys in the list</em>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Even though the operator chooses self-healing over enforcement of
synchronous replication settings, our recommendation is to plan for
synchronous replication only in clusters with 3+ instances or,
more generally, when <code>maxSyncReplicas &lt; (instances - 1)</code>.</p>
</div>
<h3 id="select-nodes-for-synchronous-replication">Select nodes for synchronous replication</h3>
<p>CloudNativePG enables you to select which PostgreSQL instances are eligible to
participate in a quorum-based synchronous replication set through anti-affinity
rules based on the node labels where the PVC holding the PGDATA and the
Postgres pod are.</p>
<div class="admonition seealso">
<p class="admonition-title">Scheduling</p>
<p>For more information on the general pod affinity and anti-affinity rules,
please check the <a href="../scheduling/">"Scheduling" section</a>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <code>.spec.postgresql.syncReplicaElectionConstraint</code> option only applies to the
legacy implementation of synchronous replication
(see <a href="./#synchronous-replication-deprecated">"Synchronous Replication (Deprecated)"</a>).</p>
</div>
<p>As an example use-case for this feature: in a cluster with a single sync
replica, we would be able to ensure the sync replica will be in a different
availability zone from the primary instance, usually identified by
the <code>topology.kubernetes.io/zone</code>
<a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone">label on a node</a>.
This would increase the robustness of the cluster in case of an outage in a
single availability zone, especially in terms of recovery point objective
(<a href="../before_you_start/#rpo">RPO</a>).</p>
<p>The idea of anti-affinity is to ensure that sync replicas that participate in
the quorum are chosen from pods running on nodes that have different values for
the selected labels (in this case, the availability zone label) then the node
where the primary is currently in execution. If no node matches such criteria,
the replicas are eligible for synchronous replication.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The self-healing enforcement still applies while defining additional
constraints for synchronous replica election
(see <a href="./#synchronous-replication">"Synchronous replication"</a>).</p>
</div>
<p>The example below shows how this can be done through the
<code>syncReplicaElectionConstraint</code> section within <code>.spec.postgresql</code>.
<code>nodeLabelsAntiAffinity</code> allows you to specify those node labels that need to be
evaluated to make sure that synchronous replication will be dynamically
configured by the operator between the current primary and the replicas which
are located on nodes having a value of the availability zone label different
from that of the node where the primary is:</p>
<pre><code class="language-yaml">spec:
  instances: 3
  postgresql:
    syncReplicaElectionConstraint:
      enabled: true
      nodeLabelsAntiAffinity:
      - topology.kubernetes.io/zone
</code></pre>
<p>As you can imagine, the availability zone is just an example, but you could
customize this behavior based on other labels that describe the node, such
as storage, CPU, or memory.</p>
<h2 id="replication-slots">Replication slots</h2>
<p><a href="https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS">Replication slots</a>
are a native PostgreSQL feature introduced in 9.4 that provides an automated way
to ensure that the primary does not remove WAL segments until all the attached
streaming replication clients have received them, and that the primary does not
remove rows which could cause a recovery conflict even when the standby is (
temporarily) disconnected.</p>
<p>A replication slot exists solely on the instance that created it, and PostgreSQL
does not replicate it on the standby servers. As a result, after a failover or a
switchover, the new primary does not contain the replication slot from the old
primary. This can create problems for the streaming replication clients that
were connected to the old primary and have lost their slot.</p>
<p>CloudNativePG provides a turn-key solution to synchronize the content of
physical replication slots from the primary to each standby, addressing two use
cases:</p>
<ul>
<li>the replication slots automatically created for the High Availability of the
  Postgres cluster (
  see <a href="#replication-slots-for-high-availability">"Replication slots for High Availability" below</a>
  for details)</li>
<li><a href="#user-defined-replication-slots">user-defined replication slots</a> created on
  the primary</li>
</ul>
<h3 id="replication-slots-for-high-availability">Replication slots for High Availability</h3>
<p>CloudNativePG fills this gap by introducing the concept of cluster-managed
replication slots, starting with high availability clusters. This feature
automatically manages physical replication slots for each hot standby replica in
the High Availability cluster, both in the primary and the standby.</p>
<p>In CloudNativePG, we use the terms:</p>
<ul>
<li><strong>Primary HA slot</strong>: a physical replication slot whose lifecycle is entirely
  managed by the current primary of the cluster and whose purpose is to map to a
  specific standby in streaming replication. Such a slot lives on the primary
  only.</li>
<li><strong>Standby HA slot</strong>: a physical replication slot for a standby whose lifecycle
  is entirely managed by another standby in the cluster, based on the content of
  the <code>pg_replication_slots</code> view in the primary, and updated at regular
  intervals using <code>pg_replication_slot_advance()</code>.</li>
</ul>
<p>This feature is enabled by default and can be disabled via configuration. For
details, please refer to the
<a href="../cloudnative-pg.v1/#postgresql-cnpg-io-v1-ReplicationSlotsConfiguration">"replicationSlots" section in the API reference</a>.
Here follows a brief description of the main options:</p>
<dl>
<dt><code>.spec.replicationSlots.highAvailability.enabled</code></dt>
<dd>if <code>true</code>, the feature is enabled (<code>true</code> is the default)</dd>
<dt><code>.spec.replicationSlots.highAvailability.slotPrefix</code></dt>
<dd>the prefix that identifies replication slots managed by the operator for this
feature (default: <code>_cnpg_</code>)</dd>
<dt><code>.spec.replicationSlots.updateInterval</code></dt>
<dd>how often the standby synchronizes the position of the local copy of the
replication slots with the position on the current primary, expressed in
seconds (default: 30)</dd>
</dl>
<p>Although it is not recommended, if you desire a different behavior, you can
customize the above options.</p>
<p>For example, the following manifest will create a cluster with replication
slots disabled.</p>
<pre><code class="language-yaml">apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: cluster-example
spec:
  instances: 3
  # Disable replication slots for HA in the cluster
  replicationSlots:
    highAvailability:
      enabled: false

  storage:
    size: 1Gi
</code></pre>
<h3 id="user-defined-replication-slots">User-Defined Replication slots</h3>
<p>Although CloudNativePG doesn't support a way to declaratively define physical
replication slots, you can still <a href="https://www.postgresql.org/docs/current/functions-admin.html#FUNCTIONS-REPLICATION">create your own slots via SQL</a>.</p>
<div class="admonition information">
<p class="admonition-title">Information</p>
<p>At the moment, we don't have any plans to manage replication slots
in a declarative way, but it might change depending on the feedback
we receive from users. The reason is that replication slots exist
for a specific purpose and each should be managed by a specific application
the oversees the entire lifecycle of the slot on the primary.</p>
</div>
<p>CloudNativePG can manage the synchronization of any user managed physical
replication slots between the primary and standbys, similarly to what it does
for the HA replication slots explained above (the only difference is that you
need to create the replication slot).</p>
<p>This feature is enabled by default (meaning that any replication slot is
synchronized), but you can disable it or further customize its behavior (for
example by excluding some slots using regular expressions) through the
<code>synchronizeReplicas</code> stanza. For example:</p>
<pre><code class="language-yaml">apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: cluster-example
spec:
  instances: 3
  replicationSlots:
    synchronizeReplicas:
      enabled: true
      excludePatterns:
      - &quot;^foo&quot;
</code></pre>
<p>For details, please refer to the
<a href="../cloudnative-pg.v1/#postgresql-cnpg-io-v1-ReplicationSlotsConfiguration">"replicationSlots" section in the API reference</a>.
Here follows a brief description of the main options:</p>
<dl>
<dt><code>.spec.replicationSlots.synchronizeReplicas.enabled</code></dt>
<dd>When true or not specified, every user-defined replication slot on the
  primary is synchronized on each standby. If changed to false, the operator will
  remove any replication slot previously created by itself on each standby.</dd>
<dt><code>.spec.replicationSlots.synchronizeReplicas.excludePatterns</code></dt>
<dd>A list of regular expression patterns to match the names of user-defined
  replication slots to be excluded from synchronization. This can be useful to
  exclude specific slots based on naming conventions.</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Users utilizing this feature should carefully monitor user-defined replication
slots to ensure they align with their operational requirements and do not
interfere with the failover process.</p>
</div>
<h3 id="synchronization-frequency">Synchronization frequency</h3>
<p>You can also control the frequency with which a standby queries the
<code>pg_replication_slots</code> view on the primary, and updates its local copy of
the replication slots, like in this example:</p>
<pre><code class="language-yaml">apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: cluster-example
spec:
  instances: 3
  # Reduce the frequency of standby HA slots updates to once every 5 minutes
  replicationSlots:
    updateInterval: 300

  storage:
    size: 1Gi
</code></pre>
<h3 id="capping-the-wal-size-retained-for-replication-slots">Capping the WAL size retained for replication slots</h3>
<p>When replication slots is enabled, you might end up running out of disk space
due to PostgreSQL trying to retain WAL files requested by a replication slot.
This might happen due to a standby that is (temporarily?) down, or lagging, or
simply an orphan replication slot.</p>
<p>Starting with PostgreSQL 13, you can take advantage of the
<a href="https://www.postgresql.org/docs/current/runtime-config-replication.html#GUC-MAX-SLOT-WAL-KEEP-SIZE"><code>max_slot_wal_keep_size</code></a>
configuration option controlling the maximum size of WAL files that replication
slots are allowed to retain in the <code>pg_wal</code> directory at checkpoint time. By
default, in PostgreSQL <code>max_slot_wal_keep_size</code> is set to <code>-1</code>, meaning that
replication slots may retain an unlimited amount of WAL files. As a result, our
recommendation is to explicitly set <code>max_slot_wal_keep_size</code>
when replication slots support is enabled. For example:</p>
<pre><code class="language-ini">  # ...
  postgresql:
    parameters:
      max_slot_wal_keep_size: &quot;10GB&quot;
  # ...
</code></pre>
<h3 id="monitoring-replication-slots">Monitoring replication slots</h3>
<p>Replication slots must be carefully monitored in your infrastructure. By default,
we provide the <code>pg_replication_slots</code> metric in our Prometheus exporter with
key information such as the name of the slot, the type, whether it is active,
the lag from the primary.</p>
<div class="admonition seealso">
<p class="admonition-title">Monitoring</p>
<p>Please refer to the <a href="../monitoring/">"Monitoring" section</a> for details on
how to monitor a CloudNativePG deployment.</p>
</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../rolling_update/" class="btn btn-neutral float-left" title="Rolling Updates"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../logical_replication/" class="btn btn-neutral float-right" title="Logical Replication">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../rolling_update/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../logical_replication/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
