<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="The CloudNativePG Contributors" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Troubleshooting - CloudNativePG v1.24</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/override.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Troubleshooting";
        var mkdocs_page_input_path = "troubleshooting.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> CloudNativePG v1.24
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">CloudNativePG</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../before_you_start/">Before You Start</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../use_cases/">Use cases</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../architecture/">Architecture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../installation_upgrade/">Installation and upgrades</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../quickstart/">Quickstart</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../image_catalog/">Image Catalog</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../bootstrap/">Bootstrap</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../database_import/">Importing Postgres databases</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../security/">Security</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../instance_manager/">Postgres instance manager</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../scheduling/">Scheduling</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../resource_management/">Resource management</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../failure_modes/">Failure Modes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../rolling_update/">Rolling Updates</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../replication/">Replication</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../backup/">Backup</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../backup_barmanobjectstore/">Backup on object stores</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../wal_archiving/">WAL archiving</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../backup_volumesnapshot/">Backup on volume snapshots</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../recovery/">Recovery</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../service_management/">Service Management</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../postgresql_conf/">PostgreSQL Configuration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../declarative_role_management/">Database Role Management</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../tablespaces/">Tablespaces</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../operator_conf/">Operator configuration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../cluster_conf/">Instance pod configuration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../storage/">Storage</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../labels_annotations/">Labels and annotations</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../monitoring/">Monitoring</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../logging/">Logging</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../certificates/">Certificates</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../ssl_connections/">Client TLS/SSL connections</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../applications/">Connecting from an application</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../connection_pooling/">Connection pooling</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../replica_cluster/">Replica clusters</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../kubernetes_upgrade/">Kubernetes Upgrade and Maintenance</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../kubectl-plugin/">Kubectl Plugin</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../failover/">Automated failover</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Troubleshooting</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#before-you-start">Before you start</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#kubernetes-environment">Kubernetes environment</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#useful-utilities">Useful utilities</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#first-steps">First steps</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#are-there-backups">Are there backups?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#emergency-backup">Emergency backup</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#logs">Logs</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#operator-information">Operator information</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#gather-more-information-about-the-operator">Gather more information about the operator</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#cluster-information">Cluster information</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#pod-information">Pod information</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#gather-and-filter-extra-information-about-postgresql-pods">Gather and filter extra information about PostgreSQL pods</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#backup-information">Backup information</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#storage-information">Storage information</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#node-information">Node information</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#conditions">Conditions</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#how-to-wait-for-a-particular-condition">How to wait for a particular condition</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#networking">Networking</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#postgresql-core-dumps">PostgreSQL core dumps</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#some-known-issues">Some known issues</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#storage-is-full">Storage is full</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#pods-are-stuck-in-pending-state">Pods are stuck in Pending state</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#replicas-out-of-sync-when-no-backup-is-configured">Replicas out of sync when no backup is configured</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cluster-stuck-in-creating-new-replica">Cluster stuck in Creating new replica</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#networking-is-impaired-by-installed-network-policies">Networking is impaired by installed Network Policies</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#error-while-bootstrapping-the-data-directory">Error while bootstrapping the data directory</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#bootstrap-job-hangs-in-running-status">Bootstrap job hangs in running status</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#replicas-take-over-two-minutes-to-reconnect-after-a-failover">Replicas take over two minutes to reconnect after a failover</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../fencing/">Fencing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../declarative_hibernation/">Declarative hibernation</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../postgis/">PostGIS</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../e2e/">End-to-End Tests</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../container_images/">Container Image Requirements</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../operator_capability_levels/">Operator capability levels</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../controller/">Custom Pod Controller</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../samples/">Examples</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../networking/">Networking</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../benchmarking/">Benchmarking</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../faq/">Frequently Asked Questions (FAQ)</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../cloudnative-pg.v1/">API Reference</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../supported_releases/">Supported releases</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../preview_version/">Preview Versions</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../release_notes/">Release notes</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Appendixes</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../appendixes/object_stores/">Appendix A - Common object stores for backups</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">CloudNativePG v1.24</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Troubleshooting</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="troubleshooting">Troubleshooting</h1>
<p>In this page, you can find some basic information on how to troubleshoot
CloudNativePG in your Kubernetes cluster deployment.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>As a Kubernetes administrator, you should have the
<a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/"><code>kubectl</code> Cheat Sheet</a> page
bookmarked!</p>
</div>
<h2 id="before-you-start">Before you start</h2>
<h3 id="kubernetes-environment">Kubernetes environment</h3>
<p>What can make a difference in a troubleshooting activity is to provide
clear information about the underlying Kubernetes system.</p>
<p>Make sure you know:</p>
<ul>
<li>the Kubernetes distribution and version you are using</li>
<li>the specifications of the nodes where PostgreSQL is running</li>
<li>as much as you can about the actual <a href="../storage/">storage</a>, including storage
  class and benchmarks you have done before going into production.</li>
<li>which relevant Kubernetes applications you are using in your cluster (i.e.
  Prometheus, Grafana, Istio, Certmanager, ...)</li>
<li>the situation of continuous backup, in particular if it's in place and working
  correctly: in case it is not, make sure you take an <a href="#emergency-backup">emergency backup</a>
  before performing any potential disrupting operation</li>
</ul>
<h3 id="useful-utilities">Useful utilities</h3>
<p>On top of the mandatory <code>kubectl</code> utility, for troubleshooting, we recommend the
following plugins/utilities to be available in your system:</p>
<ul>
<li><a href="../kubectl-plugin/"><code>cnpg</code> plugin</a> for <code>kubectl</code></li>
<li><a href="https://stedolan.github.io/jq/"><code>jq</code></a>, a lightweight and flexible command-line JSON processor</li>
<li><a href="https://www.gnu.org/software/grep/"><code>grep</code></a>, searches one or more input files
  for lines containing a match to a specified pattern. It is already available in most *nix distros.
  If you are on Windows OS, you can use <a href="https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/findstr"><code>findstr</code></a> as an alternative to <code>grep</code> or directly use <a href="https://docs.microsoft.com/en-us/windows/wsl/"><code>wsl</code></a>
  and install your preferred *nix distro and use the tools mentioned above.</li>
</ul>
<h2 id="first-steps">First steps</h2>
<p>To quickly get an overview of the cluster or installation, the <code>kubectl</code> plugin
is the primary tool to use:</p>
<ol>
<li>the <a href="../kubectl-plugin/#status">status subcommand</a> provides an overview of a
  cluster</li>
<li>the <a href="../kubectl-plugin/#report">report subcommand</a> provides the manifests
  for clusters and the operator deployment. It can also include logs using
  the <code>--logs</code> option.
  The report generated via the plugin will include the full cluster manifest.</li>
</ol>
<p>The plugin can be installed on air-gapped systems via packages.
Please refer to the <a href="../kubectl-plugin/">plugin document</a> for complete instructions.</p>
<h2 id="are-there-backups">Are there backups?</h2>
<p>After getting the cluster manifest with the plugin, you should verify if backups
are set up and working.</p>
<p>In a cluster with backups set up, you will find, in the cluster Status, the fields
<code>lastSuccessfulBackup</code> and <code>firstRecoverabilityPoint</code>. You should make sure
there is a recent <code>lastSuccessfulBackup</code>.</p>
<p>A cluster lacking the <code>.spec.backup</code> stanza won't have backups. 
An insistent message will appear in the PostgreSQL logs:</p>
<pre><code>Backup not configured, skip WAL archiving.
</code></pre>
<p>Before proceeding with troubleshooting operations, it may be advisable
to perform an emergency backup depending on your findings regarding backups.
Refer to the following section for instructions.</p>
<p>It is <strong>extremely risky</strong> to operate a production database without keeping
regular backups.</p>
<h2 id="emergency-backup">Emergency backup</h2>
<p>In some emergency situations, you might need to take an emergency logical
backup of the main <code>app</code> database.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The instructions you find below must be executed only in emergency situations
and the temporary backup files kept under the data protection policies
that are effective in your organization. The dump file is indeed stored
in the client machine that runs the <code>kubectl</code> command, so make sure that
all protections are in place and you have enough space to store the
backup file.</p>
</div>
<p>The following example shows how to take a logical backup of the <code>app</code> database
in the <code>cluster-example</code> Postgres cluster, from the <code>cluster-example-1</code> pod:</p>
<pre><code class="language-sh">kubectl exec cluster-example-1 -c postgres \
  -- pg_dump -Fc -d app &gt; app.dump
</code></pre>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can easily adapt the above command to backup your cluster, by providing
the names of the objects you have used in your environment.</p>
</div>
<p>The above command issues a <code>pg_dump</code> command in custom format, which is the most
versatile way to take <a href="https://www.postgresql.org/docs/current/app-pgdump.html">logical backups in PostgreSQL</a>.</p>
<p>The next step is to restore the database. We assume that you are operating
on a new PostgreSQL cluster that's been just initialized (so the <code>app</code> database
is empty).</p>
<p>The following example shows how to restore the above logical backup in the
<code>app</code> database of the <code>new-cluster-example</code> Postgres cluster, by connecting to
the primary (<code>new-cluster-example-1</code> pod):</p>
<pre><code class="language-sh">kubectl exec -i new-cluster-example-1 -c postgres \
  -- pg_restore --no-owner --role=app -d app --verbose &lt; app.dump
</code></pre>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The example in this section assumes that you have no other global objects
(databases and roles) to dump and restore, as per our recommendation. In case
you have multiple roles, make sure you have taken a backup using <code>pg_dumpall -g</code>
and you manually restore them in the new cluster. In case you have multiple
databases, you need to repeat the above operation one database at a time, making
sure you assign the right ownership. If you are not familiar with PostgreSQL,
we advise that you do these critical operations under the guidance of
a professional support company.</p>
</div>
<p>The above steps might be integrated into the <code>cnpg</code> plugin at some stage in the future.</p>
<h2 id="logs">Logs</h2>
<p>All resources created and managed by CloudNativePG log to standard output in
accordance with Kubernetes conventions, using <a href="../logging/">JSON format</a>.</p>
<p>While logs are typically processed at the infrastructure level and include
those from CloudNativePG, accessing logs directly from the command line
interface is critical during troubleshooting. You have three primary options
for doing so:</p>
<ul>
<li>Use the <code>kubectl logs</code> command to retrieve logs from a specific resource, and
  apply <code>jq</code> for better readability.</li>
<li>Use the <a href="../kubectl-plugin/#logs"><code>kubectl cnpg logs</code> command</a> for
  CloudNativePG-specific logging.</li>
<li>Leverage specialized open-source tools like
  <a href="https://github.com/stern/stern">stern</a>, which can aggregate logs from
  multiple resources (e.g., all pods in a PostgreSQL cluster by selecting the
  <code>cnpg.io/clusterName</code> label), filter log entries, customize output formats,
  and more.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The following sections provide examples of how to retrieve logs for various
resources when troubleshooting CloudNativePG.</p>
</div>
<h2 id="operator-information">Operator information</h2>
<p>By default, the CloudNativePG operator is installed in the
<code>cnpg-system</code> namespace in Kubernetes as a <code>Deployment</code>
(see the <a href="../installation_upgrade/#details-about-the-deployment">"Details about the deployment" section</a>
for details).</p>
<p>You can get a list of the operator pods by running:</p>
<pre><code class="language-shell">kubectl get pods -n cnpg-system
</code></pre>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Under normal circumstances, you should have one pod where the operator is
running, identified by a name starting with <code>cnpg-controller-manager-</code>.
In case you have set up your operator for high availability, you should have more entries.
Those pods are managed by a deployment named <code>cnpg-controller-manager</code>.</p>
</div>
<p>Collect the relevant information about the operator that is running in pod
<code>&lt;POD&gt;</code> with:</p>
<pre><code class="language-shell">kubectl describe pod -n cnpg-system &lt;POD&gt;
</code></pre>
<p>Then get the logs from the same pod by running:</p>
<pre><code class="language-shell">kubectl logs -n cnpg-system &lt;POD&gt;
</code></pre>
<h3 id="gather-more-information-about-the-operator">Gather more information about the operator</h3>
<p>Get logs from all pods in CloudNativePG operator Deployment
(in case you have a multi operator deployment) by running:</p>
<pre><code class="language-shell">kubectl logs -n cnpg-system \
  deployment/cnpg-controller-manager --all-containers=true
</code></pre>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can add <code>-f</code> flag to above command to follow logs in real time.</p>
</div>
<p>Save logs to a JSON file by running:</p>
<pre><code class="language-shell">kubectl logs -n cnpg-system \
  deployment/cnpg-controller-manager --all-containers=true | \
  jq -r . &gt; cnpg_logs.json
</code></pre>
<p>Get CloudNativePG operator version by using <code>kubectl-cnpg</code> plugin:</p>
<pre><code class="language-shell">kubectl-cnpg status &lt;CLUSTER&gt;
</code></pre>
<p>Output:</p>
<pre><code class="language-shell">Cluster in healthy state
Name:               cluster-example
Namespace:          default
System ID:          7044925089871458324
PostgreSQL Image:   ghcr.io/cloudnative-pg/postgresql:17.4-3
Primary instance:   cluster-example-1
Instances:          3
Ready instances:    3
Current Write LSN:  0/5000000 (Timeline: 1 - WAL File: 000000010000000000000004)

Continuous Backup status
Not configured

Streaming Replication status
Name               Sent LSN   Write LSN  Flush LSN  Replay LSN  Write Lag       Flush Lag       Replay Lag      State      Sync State  Sync Priority
----               --------   ---------  ---------  ----------  ---------       ---------       ----------      -----      ----------  -------------
cluster-example-2  0/5000000  0/5000000  0/5000000  0/5000000   00:00:00        00:00:00        00:00:00        streaming  async       0
cluster-example-3  0/5000000  0/5000000  0/5000000  0/5000000   00:00:00.10033  00:00:00.10033  00:00:00.10033  streaming  async       0

Instances status
Name               Database Size  Current LSN  Replication role  Status  QoS         Manager Version
----               -------------  -----------  ----------------  ------  ---         ---------------
cluster-example-1  33 MB          0/5000000    Primary           OK      BestEffort  1.12.0
cluster-example-2  33 MB          0/5000000    Standby (async)   OK      BestEffort  1.12.0
cluster-example-3  33 MB          0/5000060    Standby (async)   OK      BestEffort  1.12.0
</code></pre>
<h2 id="cluster-information">Cluster information</h2>
<p>You can check the status of the <code>&lt;CLUSTER&gt;</code> cluster in the <code>NAMESPACE</code>
namespace with:</p>
<pre><code class="language-shell">kubectl get cluster -n &lt;NAMESPACE&gt; &lt;CLUSTER&gt;
</code></pre>
<p>Output:</p>
<pre><code class="language-shell">NAME        AGE        INSTANCES   READY   STATUS                     PRIMARY
&lt;CLUSTER&gt;   10d4h3m    3           3       Cluster in healthy state   &lt;CLUSTER&gt;-1
</code></pre>
<p>The above example reports a healthy PostgreSQL cluster of 3 instances, all in
<em>ready</em> state, and with <code>&lt;CLUSTER&gt;-1</code> being the primary.</p>
<p>In case of unhealthy conditions, you can discover more by getting the manifest
of the <code>Cluster</code> resource:</p>
<pre><code class="language-shell">kubectl get cluster -o yaml -n &lt;NAMESPACE&gt; &lt;CLUSTER&gt;
</code></pre>
<p>Another important command to gather is the <code>status</code> one, as provided by the
<code>cnpg</code> plugin:</p>
<pre><code class="language-shell">kubectl cnpg status -n &lt;NAMESPACE&gt; &lt;CLUSTER&gt;
</code></pre>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can print more information by adding the <code>--verbose</code> option.</p>
</div>
<p>Get PostgreSQL container image version:</p>
<pre><code class="language-shell">kubectl describe cluster &lt;CLUSTER_NAME&gt; -n &lt;NAMESPACE&gt; | grep &quot;Image Name&quot;
</code></pre>
<p>Output:</p>
<pre><code class="language-shell">  Image Name:    ghcr.io/cloudnative-pg/postgresql:17.4-3
</code></pre>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Also you can use <code>kubectl-cnpg status -n &lt;NAMESPACE&gt; &lt;CLUSTER_NAME&gt;</code>
to get the same information.</p>
</div>
<h2 id="pod-information">Pod information</h2>
<p>You can retrieve the list of instances that belong to a given PostgreSQL
cluster with:</p>
<pre><code class="language-shell">kubectl get pod -l cnpg.io/cluster=&lt;CLUSTER&gt; -L role -n &lt;NAMESPACE&gt;
</code></pre>
<p>Output:</p>
<pre><code class="language-shell">NAME          READY   STATUS    RESTARTS   AGE       ROLE
&lt;CLUSTER&gt;-1   1/1     Running   0          10d4h5m   primary
&lt;CLUSTER&gt;-2   1/1     Running   0          10d4h4m   replica
&lt;CLUSTER&gt;-3   1/1     Running   0          10d4h4m   replica
</code></pre>
<p>You can check if/how a pod is failing by running:</p>
<pre><code class="language-shell">kubectl get pod -n &lt;NAMESPACE&gt; -o yaml &lt;CLUSTER&gt;-&lt;N&gt;
</code></pre>
<p>You can get all the logs for a given PostgreSQL instance with:</p>
<pre><code class="language-shell">kubectl logs -n &lt;NAMESPACE&gt; &lt;CLUSTER&gt;-&lt;N&gt;
</code></pre>
<p>If you want to limit the search to the PostgreSQL process only, you can run:</p>
<pre><code class="language-shell">kubectl logs -n &lt;NAMESPACE&gt; &lt;CLUSTER&gt;-&lt;N&gt; | \
  jq 'select(.logger==&quot;postgres&quot;) | .record.message'
</code></pre>
<p>The following example also adds the timestamp:</p>
<pre><code class="language-shell">kubectl logs -n &lt;NAMESPACE&gt; &lt;CLUSTER&gt;-&lt;N&gt; | \
  jq -r 'select(.logger==&quot;postgres&quot;) | [.ts, .record.message] | @csv'
</code></pre>
<p>If the timestamp is displayed in Unix Epoch time, you can convert it to a user-friendly format:</p>
<pre><code class="language-shell">kubectl logs -n &lt;NAMESPACE&gt; &lt;CLUSTER&gt;-&lt;N&gt; | \
  jq -r 'select(.logger==&quot;postgres&quot;) | [(.ts|strflocaltime(&quot;%Y-%m-%dT%H:%M:%S %Z&quot;)), .record.message] | @csv'
</code></pre>
<h3 id="gather-and-filter-extra-information-about-postgresql-pods">Gather and filter extra information about PostgreSQL pods</h3>
<p>Check logs from a specific pod that has crashed:</p>
<pre><code class="language-shell">kubectl logs -n &lt;NAMESPACE&gt; --previous &lt;CLUSTER&gt;-&lt;N&gt;
</code></pre>
<p>Get FATAL errors from a specific PostgreSQL pod:</p>
<pre><code class="language-shell">kubectl logs -n &lt;NAMESPACE&gt; &lt;CLUSTER&gt;-&lt;N&gt; | \
  jq -r '.record | select(.error_severity == &quot;FATAL&quot;)'
</code></pre>
<p>Output:</p>
<pre><code class="language-json">{
  &quot;log_time&quot;: &quot;2021-11-08 14:07:44.520 UTC&quot;,
  &quot;user_name&quot;: &quot;streaming_replica&quot;,
  &quot;process_id&quot;: &quot;68&quot;,
  &quot;connection_from&quot;: &quot;10.244.0.10:60616&quot;,
  &quot;session_id&quot;: &quot;61892f30.44&quot;,
  &quot;session_line_num&quot;: &quot;1&quot;,
  &quot;command_tag&quot;: &quot;startup&quot;,
  &quot;session_start_time&quot;: &quot;2021-11-08 14:07:44 UTC&quot;,
  &quot;virtual_transaction_id&quot;: &quot;3/75&quot;,
  &quot;transaction_id&quot;: &quot;0&quot;,
  &quot;error_severity&quot;: &quot;FATAL&quot;,
  &quot;sql_state_code&quot;: &quot;28000&quot;,
  &quot;message&quot;: &quot;role \&quot;streaming_replica\&quot; does not exist&quot;,
  &quot;backend_type&quot;: &quot;walsender&quot;
}
</code></pre>
<p>Filter PostgreSQL DB error messages in logs for a specific pod:</p>
<pre><code class="language-shell">kubectl logs -n &lt;NAMESPACE&gt; &lt;CLUSTER&gt;-&lt;N&gt; | jq -r '.err | select(. != null)'
</code></pre>
<p>Output:</p>
<pre><code class="language-shell">dial unix /controller/run/.s.PGSQL.5432: connect: no such file or directory
</code></pre>
<p>Get messages matching <code>err</code> word from a specific pod:</p>
<pre><code class="language-shell">kubectl logs -n &lt;NAMESPACE&gt; &lt;CLUSTER&gt;-&lt;N&gt; | jq -r '.msg' | grep &quot;err&quot;
</code></pre>
<p>Output:</p>
<pre><code class="language-shell">2021-11-08 14:07:39.610 UTC [15] LOG:  ending log output to stderr
</code></pre>
<p>Get all logs from PostgreSQL process from a specific pod:</p>
<pre><code class="language-shell">kubectl logs -n &lt;NAMESPACE&gt; &lt;CLUSTER&gt;-&lt;N&gt; | \
  jq -r '. | select(.logger == &quot;postgres&quot;) | select(.msg != &quot;record&quot;) | .msg'
</code></pre>
<p>Output:</p>
<pre><code class="language-shell">2021-11-08 14:07:52.591 UTC [16] LOG:  redirecting log output to logging collector process
2021-11-08 14:07:52.591 UTC [16] HINT:  Future log output will appear in directory &quot;/controller/log&quot;.
2021-11-08 14:07:52.591 UTC [16] LOG:  ending log output to stderr
2021-11-08 14:07:52.591 UTC [16] HINT:  Future log output will go to log destination &quot;csvlog&quot;.
</code></pre>
<p>Get pod logs filtered by fields with values and join them separated by <code>|</code> running:</p>
<pre><code class="language-shell">kubectl logs -n &lt;NAMESPACE&gt; &lt;CLUSTER&gt;-&lt;N&gt; | \
  jq -r '[.level, .ts, .logger, .msg] | join(&quot; | &quot;)'
</code></pre>
<p>Output:</p>
<pre><code class="language-shell">info | 1636380469.5728037 | wal-archive | Backup not configured, skip WAL archiving
info | 1636383566.0664876 | postgres | record
</code></pre>
<h2 id="backup-information">Backup information</h2>
<p>You can list the backups that have been created for a named cluster with:</p>
<pre><code class="language-shell">kubectl get backup -l cnpg.io/cluster=&lt;CLUSTER&gt;
</code></pre>
<h2 id="storage-information">Storage information</h2>
<p>Sometimes is useful to double-check the StorageClass used by the cluster to have
some more context during investigations or troubleshooting, like this:</p>
<pre><code class="language-shell">STORAGECLASS=$(kubectl get pvc &lt;POD&gt; -o jsonpath='{.spec.storageClassName}')
kubectl get storageclasses $STORAGECLASS -o yaml
</code></pre>
<p>We are taking the StorageClass from one of the cluster pod here since often
clusters are created using the default StorageClass.</p>
<h2 id="node-information">Node information</h2>
<p>Kubernetes nodes is where ultimately PostgreSQL pods will be running. It's
strategically important to know as much as we can about them.</p>
<p>You can get the list of nodes in your Kubernetes cluster with:</p>
<pre><code class="language-shell"># look at the worker nodes and their status
kubectl get nodes -o wide
</code></pre>
<p>Additionally, you can gather the list of nodes where the pods of a given
cluster are running with:</p>
<pre><code class="language-shell">kubectl get pod -l cnpg.io/cluster=&lt;CLUSTER&gt; \
  -L role -n &lt;NAMESPACE&gt; -o wide
</code></pre>
<p>The latter is important to understand where your pods are distributed - very
useful if you are using <a href="../scheduling/">affinity/anti-affinity rules and/or tolerations</a>.</p>
<h2 id="conditions">Conditions</h2>
<p>Like many native kubernetes
objects <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions">like here</a>, 
Cluster exposes <code>status.conditions</code> as well. This allows one to 'wait' for a particular 
event to occur instead of relying on the overall cluster health state. Available conditions as of now are:</p>
<ul>
<li>LastBackupSucceeded</li>
<li>ContinuousArchiving</li>
<li>Ready</li>
</ul>
<p><code>LastBackupSucceeded</code> is reporting the status of the latest backup. If set to <code>True</code> the
last backup has been taken correctly, it is set to <code>False</code> otherwise.</p>
<p><code>ContinuousArchiving</code> is reporting the status of the WAL archiving. If set to <code>True</code> the
last WAL archival process has been terminated correctly, it is set to <code>False</code> otherwise.</p>
<p><code>Ready</code> is <code>True</code> when the cluster has the number of instances specified by the user
and the primary instance is ready. This condition can be used in scripts to wait for
the cluster to be created.</p>
<h3 id="how-to-wait-for-a-particular-condition">How to wait for a particular condition</h3>
<ul>
<li>Backup:</li>
</ul>
<pre><code class="language-bash">$ kubectl wait --for=condition=LastBackupSucceeded cluster/&lt;CLUSTER-NAME&gt; -n &lt;NAMESPACE&gt;
</code></pre>
<ul>
<li>ContinuousArchiving:</li>
</ul>
<pre><code class="language-bash">$ kubectl wait --for=condition=ContinuousArchiving cluster/&lt;CLUSTER-NAME&gt; -n &lt;NAMESPACE&gt;
</code></pre>
<ul>
<li>Ready (Cluster is ready or not):</li>
</ul>
<pre><code class="language-bash">$ kubectl wait --for=condition=Ready cluster/&lt;CLUSTER-NAME&gt; -n &lt;NAMESPACE&gt;
</code></pre>
<p>Below is a snippet of a <code>cluster.status</code> that contains a failing condition.</p>
<pre><code class="language-bash">$ kubectl get cluster/&lt;cluster-name&gt; -o yaml
.
.
.
  status:
    conditions:
    - message: 'unexpected failure invoking barman-cloud-wal-archive: exit status
        2'
      reason: ContinuousArchivingFailing
      status: &quot;False&quot;
      type: ContinuousArchiving

    - message: exit status 2
      reason: LastBackupFailed
      status: &quot;False&quot;
      type: LastBackupSucceeded

    - message: Cluster Is Not Ready
      reason: ClusterIsNotReady
      status: &quot;False&quot;
      type: Ready


</code></pre>
<h2 id="networking">Networking</h2>
<p>CloudNativePG requires basic networking and connectivity in place.
You can find more information in the <a href="../networking/">networking</a> section.</p>
<p>If installing CloudNativePG in an existing environment, there might be
network policies in place, or other network configuration made specifically
for the cluster, which could have an impact on the required connectivity
between the operator and the cluster pods and/or the between the pods.</p>
<p>You can look for existing network policies with the following command:</p>
<pre><code class="language-sh">kubectl get networkpolicies
</code></pre>
<p>There might be several network policies set up by the Kubernetes network
administrator.</p>
<pre><code class="language-sh">$ kubectl get networkpolicies                       
NAME                   POD-SELECTOR                      AGE
allow-prometheus       cnpg.io/cluster=cluster-example   47m
default-deny-ingress   &lt;none&gt;                            57m
</code></pre>
<h2 id="postgresql-core-dumps">PostgreSQL core dumps</h2>
<p>Although rare, PostgreSQL can sometimes crash and generate a core dump
in the <code>PGDATA</code> folder. When that happens, normally it is a bug in PostgreSQL
(and most likely it has already been solved - this is why it is important
to always run the latest minor version of PostgreSQL).</p>
<p>CloudNativePG allows you to control what to include in the core dump through
the <code>cnpg.io/coredumpFilter</code> annotation.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Please refer to <a href="../labels_annotations/">"Labels and annotations"</a>
for more details on the standard annotations that CloudNativePG provides.</p>
</div>
<p>By default, the <code>cnpg.io/coredumpFilter</code> is set to <code>0x31</code> in order to
exclude shared memory segments from the dump, as this is the safest
approach in most cases.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Please refer to
<a href="https://docs.kernel.org/filesystems/proc.html#proc-pid-coredump-filter-core-dump-filtering-settings">"Core dump filtering settings" section of "The <code>/proc</code> Filesystem" page of the Linux Kernel documentation</a>.
for more details on how to set the bitmask that controls the core dump filter.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Beware that this setting only takes effect during Pod startup and that changing
the annotation doesn't trigger an automated rollout of the instances.</p>
</div>
<p>Although you might not personally be involved in inspecting core dumps,
you might be asked to provide them so that a Postgres expert can look
into them. First, verify that you have a core dump in the <code>PGDATA</code>
directory with the following command (please run it against the
correct pod where the Postgres instance is running):</p>
<pre><code class="language-sh">kubectl exec -ti POD -c postgres \
  -- find /var/lib/postgresql/data/pgdata -name 'core.*'
</code></pre>
<p>Under normal circumstances, this should return an empty set. Suppose, for
example, that we have a core dump file:</p>
<pre><code>/var/lib/postgresql/data/pgdata/core.14177
</code></pre>
<p>Once you have verified the space on disk is sufficient, you can collect the
core dump on your machine through <code>kubectl cp</code> as follows:</p>
<pre><code class="language-sh">kubectl cp POD:/var/lib/postgresql/data/pgdata/core.14177 core.14177
</code></pre>
<p>You now have the file. Make sure you free the space on the server by
removing the core dumps.</p>
<h2 id="some-known-issues">Some known issues</h2>
<h3 id="storage-is-full">Storage is full</h3>
<p>In case the storage is full, the PostgreSQL pods will not be able to write new
data, or, in case of the disk containing the WAL segments being full, PostgreSQL
will shut down.</p>
<p>If you see messages in the logs about the disk being full, you should increase
the size of the affected PVC. You can do this by editing the PVC and changing
the <code>spec.resources.requests.storage</code> field. After that, you should also update
the Cluster resource with the new size to apply the same change to all the pods.
Please look at the <a href="../storage/#volume-expansion">"Volume expansion" section</a> in the documentation.</p>
<p>If the space for WAL segments is exhausted, the pod will be crash-looping and
the cluster status will report <code>Not enough disk space</code>. Increasing the size in
the PVC and then in the Cluster resource will solve the issue. See also
the <a href="../instance_manager/#disk-full-failure">"Disk Full Failure" section</a></p>
<h3 id="pods-are-stuck-in-pending-state">Pods are stuck in <code>Pending</code> state</h3>
<p>In case a Cluster's instance is stuck in the <code>Pending</code> phase, you should check
the pod's <code>Events</code> section to get an idea of the reasons behind this:</p>
<pre><code class="language-shell">kubectl describe pod -n &lt;NAMESPACE&gt; &lt;POD&gt;
</code></pre>
<p>Some of the possible causes for this are:</p>
<ul>
<li>No nodes are matching the <code>nodeSelector</code></li>
<li>Tolerations are not correctly configured to match the nodes' taints</li>
<li>No nodes are available at all: this could also be related to
  <code>cluster-autoscaler</code> hitting some limits, or having some temporary issues</li>
</ul>
<p>In this case, it could also be useful to check events in the namespace:</p>
<pre><code class="language-shell">kubectl get events -n &lt;NAMESPACE&gt;
# list events in chronological order
kubectl get events -n &lt;NAMESPACE&gt; --sort-by=.metadata.creationTimestamp
</code></pre>
<h3 id="replicas-out-of-sync-when-no-backup-is-configured">Replicas out of sync when no backup is configured</h3>
<p>Sometimes replicas might be switched off for a bit of time due to maintenance
reasons (think of when a Kubernetes nodes is drained). In case your cluster
does not have backup configured, when replicas come back up, they might
require a WAL file that is not present anymore on the primary (having been
already recycled according to the WAL management policies as mentioned in
<a href="../postgresql_conf/#the-postgresql-section">"The <code>postgresql</code> section"</a>), and
fall out of synchronization.</p>
<p>Similarly, when <code>pg_rewind</code> might require a WAL file that is not present
anymore in the former primary, reporting <code>pg_rewind: error: could not open file</code>.</p>
<p>In these cases, pods cannot become ready anymore, and you are required to delete
the PVC and let the operator rebuild the replica.</p>
<p>If you rely on dynamically provisioned Persistent Volumes, and you are confident
in deleting the PV itself, you can do so with:</p>
<pre><code class="language-shell">PODNAME=&lt;POD&gt;
VOLNAME=$(kubectl get pv -o json | \
  jq -r '.items[]|select(.spec.claimRef.name=='\&quot;$PODNAME\&quot;')|.metadata.name')

kubectl delete pod/$PODNAME pvc/$PODNAME pvc/$PODNAME-wal pv/$VOLNAME
</code></pre>
<h3 id="cluster-stuck-in-creating-new-replica">Cluster stuck in <code>Creating new replica</code></h3>
<p>Cluster is stuck in "Creating a new replica", while pod logs don't show
relevant problems.
This has been found to be related to the next issue
<a href="#networking-is-impaired-by-installed-network-policies">on connectivity</a>.
Networking issues are reflected in the status column as follows:</p>
<pre><code class="language-text">Instance Status Extraction Error: HTTP communication issue
</code></pre>
<h3 id="networking-is-impaired-by-installed-network-policies">Networking is impaired by installed Network Policies</h3>
<p>As pointed out in the <a href="#networking">networking section</a>, local network policies
could prevent some of the required connectivity.</p>
<p>A tell-tale sign that connectivity is impaired is the presence in the operator
logs of messages like:</p>
<pre><code class="language-text">&quot;Cannot extract Pod status&quot;, […snipped…] &quot;Get \&quot;http://&lt;pod IP&gt;:8000/pg/status\&quot;: dial tcp &lt;pod IP&gt;:8000: i/o timeout&quot;
</code></pre>
<p>You should list the network policies, and look for any policies restricting
connectivity.</p>
<pre><code class="language-sh">$ kubectl get networkpolicies                       
NAME                   POD-SELECTOR                      AGE
allow-prometheus       cnpg.io/cluster=cluster-example   47m
default-deny-ingress   &lt;none&gt;                            57m
</code></pre>
<p>For example, in the listing above, <code>default-deny-ingress</code> seems a likely culprit.
You can drill into it:</p>
<pre><code class="language-sh">$ kubectl get networkpolicies default-deny-ingress -o yaml
&lt;…snipped…&gt;
spec:
  podSelector: {}
  policyTypes:
  - Ingress
</code></pre>
<p>In the <a href="../networking/">networking page</a> you can find a network policy file
that you can customize to create a <code>NetworkPolicy</code> explicitly allowing the
operator to connect cross-namespace to cluster pods.</p>
<h3 id="error-while-bootstrapping-the-data-directory">Error while bootstrapping the data directory</h3>
<p>If your Cluster's initialization job crashes with a "Bus error (core dumped)
child process exited with exit code 135", you likely need to fix the Cluster
hugepages settings.</p>
<p>The reason is the incomplete support of hugepages in the cgroup v1 that should
be fixed in v2. For more information, check the PostgreSQL <a href="https://www.postgresql.org/message-id/17757-dbdfc1f1c954a6db%40postgresql.org">BUG #17757: Not
honoring huge_pages setting during initdb causes DB crash in
Kubernetes</a>.</p>
<p>To check whether hugepages are enabled, run <code>grep HugePages /proc/meminfo</code> on
the Kubernetes node and check if hugepages are present, their size, and how many
are free.</p>
<p>If the hugepages are present, you need to configure how much hugepages memory
every PostgreSQL pod should have available.</p>
<p>For example:</p>
<pre><code class="language-yaml">  postgresql:
    parameters:
      shared_buffers: &quot;128MB&quot;

  resources:
    requests:
      memory: &quot;512Mi&quot;
    limits:
      hugepages-2Mi: &quot;512Mi&quot;
</code></pre>
<p>Please remember that you must have enough hugepages memory available to schedule
every Pod in the Cluster (in the example above, at least 512MiB per Pod must be
free).</p>
<h3 id="bootstrap-job-hangs-in-running-status">Bootstrap job hangs in running status</h3>
<p>If your Cluster's initialization job hangs while in <code>Running</code> status with the
message:
"error while waiting for the API server to be reachable", you probably have
a network issue preventing communication with the Kubernetes API server.
Initialization jobs (like most of jobs) need to access the Kubernetes
API. Please check your networking.</p>
<p>Another possible cause is when you have sidecar injection configured. Sidecars
such as Istio may make the network temporarily unavailable during startup. If
you have sidecar injection enabled, retry with injection disabled.</p>
<h3 id="replicas-take-over-two-minutes-to-reconnect-after-a-failover">Replicas take over two minutes to reconnect after a failover</h3>
<p>When the primary instance fails, the operator promotes the most advanced standby
to the primary role. Other standby instances then attempt to reconnect to the
<code>-rw</code> service for replication. However, during this reconnection process,
<code>kube-proxy</code> may not have updated its routing information yet. As a result, the
initial <code>SYN</code> packet sent by the standby instances might fail to reach its
intended destination.</p>
<p>If the network is configured to silently drop packets instead of rejecting them,
standby instances will not receive a response and will retry the connection
after an exponential backoff period. On Linux systems, the default value for the
<code>tcp_syn_retries</code> kernel parameter is 6, meaning the system will attempt to
establish the connection for approximately 127 seconds before giving up. This
prolonged retry period can significantly delay the reconnection process.
For more details, consult the
<a href="https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt">tcp_syn_retries documentation</a>.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../failover/" class="btn btn-neutral float-left" title="Automated failover"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../fencing/" class="btn btn-neutral float-right" title="Fencing">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../failover/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../fencing/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
