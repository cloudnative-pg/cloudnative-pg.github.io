<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="The CloudNativePG Contributors" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Kubectl Plugin - CloudNativePG v1.24</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/override.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Kubectl Plugin";
        var mkdocs_page_input_path = "kubectl-plugin.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> CloudNativePG v1.24
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">CloudNativePG</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../before_you_start/">Before You Start</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../use_cases/">Use cases</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../architecture/">Architecture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../installation_upgrade/">Installation and upgrades</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../quickstart/">Quickstart</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../image_catalog/">Image Catalog</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../bootstrap/">Bootstrap</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../database_import/">Importing Postgres databases</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../security/">Security</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../instance_manager/">Postgres instance manager</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../scheduling/">Scheduling</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../resource_management/">Resource management</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../failure_modes/">Failure Modes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../rolling_update/">Rolling Updates</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../replication/">Replication</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../backup/">Backup</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../backup_barmanobjectstore/">Backup on object stores</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../wal_archiving/">WAL archiving</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../backup_volumesnapshot/">Backup on volume snapshots</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../recovery/">Recovery</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../service_management/">Service Management</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../postgresql_conf/">PostgreSQL Configuration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../declarative_role_management/">Database Role Management</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../tablespaces/">Tablespaces</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../operator_conf/">Operator configuration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../cluster_conf/">Instance pod configuration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../storage/">Storage</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../labels_annotations/">Labels and annotations</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../monitoring/">Monitoring</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../logging/">Logging</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../certificates/">Certificates</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../ssl_connections/">Client TLS/SSL connections</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../applications/">Connecting from an application</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../connection_pooling/">Connection pooling</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../replica_cluster/">Replica clusters</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../kubernetes_upgrade/">Kubernetes Upgrade and Maintenance</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Kubectl Plugin</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#install">Install</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#via-the-installation-script">Via the installation script</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-the-debian-or-redhat-packages">Using the Debian or RedHat packages</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#debian-packages">Debian packages</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#rpm-packages">RPM packages</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-the-arch-linux-user-repository-aur-package">Using the Arch Linux User Repository (AUR) Package</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-krew">Using Krew</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-homebrew">Using Homebrew</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#supported-architectures">Supported Architectures</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#configuring-auto-completion">Configuring auto-completion</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#use">Use</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#generation-of-installation-manifests">Generation of installation manifests</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#status">Status</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#promote">Promote</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#certificates">Certificates</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#restart">Restart</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#reload">Reload</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#maintenance">Maintenance</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#report">Report</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#report-operator">report Operator</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#report-cluster">report Cluster</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#logs">Logs</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#cluster-logs">Cluster logs</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#pretty">Pretty</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#destroy">Destroy</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cluster-hibernation">Cluster hibernation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#benchmarking-the-database-with-pgbench">Benchmarking the database with pgbench</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#benchmarking-the-storage-with-fio">Benchmarking the storage with fio</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#requesting-a-new-physical-backup">Requesting a new physical backup</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#launching-psql">Launching psql</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#snapshotting-a-postgres-cluster">Snapshotting a Postgres cluster</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-pgadmin4-for-evaluationdemonstration-purposes-only">Using pgAdmin4 for evaluation/demonstration purposes only</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#logical-replication-publications">Logical Replication Publications</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#creating-a-new-publication">Creating a new publication</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#dropping-a-publication">Dropping a publication</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#logical-replication-subscriptions">Logical Replication Subscriptions</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#creating-a-new-subscription">Creating a new subscription</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#dropping-a-subscription">Dropping a subscription</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#synchronizing-sequences">Synchronizing sequences</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#integration-with-k9s">Integration with K9s</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#permissions-required-by-the-plugin">Permissions required by the plugin</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#role-examples">Role examples</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../failover/">Automated failover</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../troubleshooting/">Troubleshooting</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../fencing/">Fencing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../declarative_hibernation/">Declarative hibernation</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../postgis/">PostGIS</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../e2e/">End-to-End Tests</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../container_images/">Container Image Requirements</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../operator_capability_levels/">Operator capability levels</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../controller/">Custom Pod Controller</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../samples/">Examples</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../networking/">Networking</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../benchmarking/">Benchmarking</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../faq/">Frequently Asked Questions (FAQ)</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../cloudnative-pg.v1/">API Reference</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../supported_releases/">Supported releases</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../preview_version/">Preview Versions</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../release_notes/">Release notes</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Appendixes</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../appendixes/object_stores/">Appendix A - Common object stores for backups</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">CloudNativePG v1.24</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Kubectl Plugin</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="kubectl-plugin">Kubectl Plugin</h1>
<p>CloudNativePG provides a plugin for <code>kubectl</code> to manage a cluster in Kubernetes.</p>
<h2 id="install">Install</h2>
<p>You can install the <code>cnpg</code> plugin using a variety of methods.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For air-gapped systems, installation via package managers, using previously
downloaded files, may be a good option.</p>
</div>
<h3 id="via-the-installation-script">Via the installation script</h3>
<pre><code class="language-sh">curl -sSfL \
  https://github.com/cloudnative-pg/cloudnative-pg/raw/main/hack/install-cnpg-plugin.sh | \
  sudo sh -s -- -b /usr/local/bin
</code></pre>
<h3 id="using-the-debian-or-redhat-packages">Using the Debian or RedHat packages</h3>
<p>In the
<a href="https://github.com/cloudnative-pg/cloudnative-pg/releases">releases section of the GitHub repository</a>,
you can navigate to any release of interest (pick the same or newer release
than your CloudNativePG operator), and in it you will find an <strong>Assets</strong>
section. In that section are pre-built packages for a variety of systems.
As a result, you can follow standard practices and instructions to install
them in your systems.</p>
<h4 id="debian-packages">Debian packages</h4>
<p>For example, let's install the 1.24.3 release of the plugin, for an Intel based
64 bit server. First, we download the right <code>.deb</code> file.</p>
<pre><code class="language-sh">wget https://github.com/cloudnative-pg/cloudnative-pg/releases/download/v1.24.3/kubectl-cnpg_1.24.3_linux_x86_64.deb \
  --output-document kube-plugin.deb
</code></pre>
<p>Then, with superuser privileges, install from the local file using <code>dpkg</code>:</p>
<pre><code class="language-console">$ sudo dpkg -i kube-plugin.deb
Selecting previously unselected package cnpg.
(Reading database ... 6688 files and directories currently installed.)
Preparing to unpack kube-plugin.deb ...
Unpacking cnpg (1.24.3) ...
Setting up cnpg (1.24.3) ...
</code></pre>
<h4 id="rpm-packages">RPM packages</h4>
<p>As in the example for <code>.rpm</code> packages, let's install the 1.24.3 release for an
Intel 64 bit machine. Note the <code>--output</code> flag to provide a file name.</p>
<pre><code class="language-sh">curl -L https://github.com/cloudnative-pg/cloudnative-pg/releases/download/v1.24.3/kubectl-cnpg_1.24.3_linux_x86_64.rpm \
  --output kube-plugin.rpm
</code></pre>
<p>Then, with superuser privileges, install with <code>yum</code>, and you're ready to use:</p>
<pre><code class="language-console">$ sudo yum --disablerepo=* localinstall kube-plugin.rpm
Failed to set locale, defaulting to C.UTF-8
Dependencies resolved.
====================================================================================================
 Package            Architecture         Version                   Repository                  Size
====================================================================================================
Installing:
 cnpg               x86_64               1.24.3                  @commandline                20 M

Transaction Summary
====================================================================================================
Install  1 Package

Total size: 20 M
Installed size: 78 M
Is this ok [y/N]: y
</code></pre>
<h3 id="using-the-arch-linux-user-repository-aur-package">Using the Arch Linux User Repository (AUR) Package</h3>
<p>To install the plugin from the <a href="https://aur.archlinux.org/packages/kubectl-cnpg">AUR</a>, follow these steps:</p>
<pre><code class="language-sh">git clone https://aur.archlinux.org/kubectl-cnpg.git
cd kubectl-cnpg
makepkg -si
</code></pre>
<p>Or use your favorite AUR-helper, for example <a href="https://github.com/Morganamilo/paru">paru</a>:</p>
<pre><code class="language-sh">paru -S kubectl-cnpg
</code></pre>
<h3 id="using-krew">Using Krew</h3>
<p>If you already have <a href="https://krew.sigs.k8s.io/">Krew</a> installed, you can simply
run:</p>
<pre><code class="language-sh">kubectl krew install cnpg
</code></pre>
<p>When a new version of the plugin is released, you can update the existing
installation with:</p>
<pre><code class="language-sh">kubectl krew update
kubectl krew upgrade cnpg
</code></pre>
<h3 id="using-homebrew">Using Homebrew</h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please note that the Homebrew community manages the availability of the <a href="https://formulae.brew.sh/formula/kubectl-cnpg">kubectl-cnpg plugin on Homebrew</a>.</p>
</div>
<p>If you already have <a href="https://brew.sh/">Homebrew</a> installed, you can simply
run:</p>
<pre><code class="language-sh">brew install kubectl-cnpg
</code></pre>
<p>When a new version of the plugin is released, you can update the existing
installation with:</p>
<pre><code class="language-sh">brew update
brew upgrade kubectl-cnpg
</code></pre>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Auto-completion for the kubectl plugin is already managed by Homebrew.
There's no need to create the <code>kubectl_complete-cnpg</code> script mentioned below.</p>
</div>
<h3 id="supported-architectures">Supported Architectures</h3>
<p>CloudNativePG Plugin is currently built for the following
operating system and architectures:</p>
<ul>
<li>Linux<ul>
<li>amd64</li>
<li>arm 5/6/7</li>
<li>arm64</li>
<li>s390x</li>
<li>ppc64le</li>
</ul>
</li>
<li>macOS<ul>
<li>amd64</li>
<li>arm64</li>
</ul>
</li>
<li>Windows<ul>
<li>386</li>
<li>amd64</li>
<li>arm 5/6/7</li>
<li>arm64</li>
</ul>
</li>
</ul>
<h3 id="configuring-auto-completion">Configuring auto-completion</h3>
<p>To configure auto-completion for the plugin, a helper shell script needs to be
installed into your current PATH. Assuming the latter contains <code>/usr/local/bin</code>,
this can be done with the following commands:</p>
<pre><code class="language-sh">cat &gt; kubectl_complete-cnpg &lt;&lt;EOF
#!/usr/bin/env sh

# Call the __complete command passing it all arguments
kubectl cnpg __complete &quot;\$@&quot;
EOF

chmod +x kubectl_complete-cnpg

# Important: the following command may require superuser permission
sudo mv kubectl_complete-cnpg /usr/local/bin
</code></pre>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The name of the script needs to be exactly the one provided since it's used by the kubectl auto-complete process</p>
</div>
<h2 id="use">Use</h2>
<p>Once the plugin is installed and deployed, you can start using it like this:</p>
<pre><code class="language-sh">kubectl cnpg COMMAND [ARGS...]
</code></pre>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The plugin automatically detects if the standard output channel is connected to a terminal.
In such cases, it may add ANSI colors to the command output. To disable colors, use the
<code>--color=never</code> option with the command.</p>
</div>
<h3 id="generation-of-installation-manifests">Generation of installation manifests</h3>
<p>The <code>cnpg</code> plugin can be used to generate the YAML manifest for the
installation of the operator. This option would typically be used if you want
to override some default configurations such as number of replicas,
installation namespace, namespaces to watch, and so on.</p>
<p>For details and available options, run:</p>
<pre><code class="language-sh">kubectl cnpg install generate --help
</code></pre>
<p>The main options are:</p>
<ul>
<li><code>-n</code>: specifies the namespace in which to install the operator (default:
  <code>cnpg-system</code>).</li>
<li><code>--control-plane</code>: if set to true, the operator deployment will include a
  toleration and affinity for <code>node-role.kubernetes.io/control-plane</code>.</li>
<li><code>--replicas</code>: sets the number of replicas in the deployment.</li>
<li><code>--watch-namespace</code>: specifies a comma-separated list of namespaces to watch
  (default: all namespaces).</li>
<li><code>--version</code>: defines the minor version of the operator to be installed, such
  as <code>1.23</code>. If a minor version is specified, the plugin installs the latest
  patch version of that minor version. If no version is supplied, the plugin
  installs the latest <code>MAJOR.MINOR.PATCH</code> version of the operator.</li>
</ul>
<p>An example of the <code>generate</code> command, which will generate a YAML manifest that
will install the operator, is as follows:</p>
<pre><code class="language-sh">kubectl cnpg install generate \
  -n king \
  --version 1.23 \
  --replicas 3 \
  --watch-namespace &quot;albert, bb, freddie&quot; \
  &gt; operator.yaml
</code></pre>
<p>The flags in the above command have the following meaning:
- <code>-n king</code> install the CNPG operator into the <code>king</code> namespace
- <code>--version 1.23</code> install the latest patch version for minor version 1.23
- <code>--replicas 3</code> install the operator with 3 replicas
- <code>--watch-namespace "albert, bb, freddie"</code> have the operator watch for
  changes in the <code>albert</code>, <code>bb</code> and <code>freddie</code> namespaces only</p>
<h3 id="status">Status</h3>
<p>The <code>status</code> command provides an overview of the current status of your
cluster, including:</p>
<ul>
<li><strong>general information</strong>: name of the cluster, PostgreSQL's system ID, number of
  instances, current timeline and position in the WAL</li>
<li><strong>backup</strong>: point of recoverability, and WAL archiving status as returned by
  the <code>pg_stat_archiver</code> view from the primary - or designated primary in the
  case of a replica cluster</li>
<li><strong>streaming replication</strong>: information taken directly from the <code>pg_stat_replication</code>
  view on the primary instance</li>
<li><strong>instances</strong>: information about each Postgres instance, taken directly by each
  instance manager; in the case of a standby, the <code>Current LSN</code> field corresponds
  to the latest write-ahead log location that has been replayed during recovery
  (replay LSN).</li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The status information above is taken at different times and at different
locations, resulting in slightly inconsistent returned values. For example,
the <code>Current Write LSN</code> location in the main header, might be different
from the <code>Current LSN</code> field in the instances status as it is taken at
two different time intervals.</p>
</div>
<pre><code class="language-sh">kubectl cnpg status sandbox
</code></pre>
<pre><code class="language-output">Cluster Summary
Name:                default/sandbox
System ID:           7423474350493388827
PostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:17.0
Primary instance:    sandbox-1
Primary start time:  2024-10-08 18:31:57 +0000 UTC (uptime 1m14s)
Status:              Cluster in healthy state
Instances:           3
Ready instances:     3
Size:                126M
Current Write LSN:   0/604DE38 (Timeline: 1 - WAL File: 000000010000000000000006)

Continuous Backup status
Not configured

Streaming Replication status
Replication Slots Enabled
Name       Sent LSN   Write LSN  Flush LSN  Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot
----       --------   ---------  ---------  ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------
sandbox-2  0/604DE38  0/604DE38  0/604DE38  0/604DE38   00:00:00   00:00:00   00:00:00    streaming  async       0              active
sandbox-3  0/604DE38  0/604DE38  0/604DE38  0/604DE38   00:00:00   00:00:00   00:00:00    streaming  async       0              active

Instances status
Name       Current LSN  Replication role  Status  QoS         Manager Version  Node
----       -----------  ----------------  ------  ---         ---------------  ----
sandbox-1  0/604DE38    Primary           OK      BestEffort  1.24.3           k8s-eu-worker
sandbox-2  0/604DE38    Standby (async)   OK      BestEffort  1.24.3           k8s-eu-worker2
sandbox-3  0/604DE38    Standby (async)   OK      BestEffort  1.24.3           k8s-eu-worker
</code></pre>
<p>If you require more detailed status information, use the <code>--verbose</code> option (or
<code>-v</code> for short). The level of detail increases each time the flag is repeated:</p>
<pre><code class="language-sh">kubectl cnpg status sandbox --verbose
</code></pre>
<pre><code class="language-output">Cluster Summary
Name:                default/sandbox
System ID:           7423474350493388827
PostgreSQL Image:    ghcr.io/cloudnative-pg/postgresql:17.0
Primary instance:    sandbox-1
Primary start time:  2024-10-08 18:31:57 +0000 UTC (uptime 2m4s)
Status:              Cluster in healthy state
Instances:           3
Ready instances:     3
Size:                126M
Current Write LSN:   0/6053720 (Timeline: 1 - WAL File: 000000010000000000000006)

Continuous Backup status
Not configured

Physical backups
No running physical backups found

Streaming Replication status
Replication Slots Enabled
Name       Sent LSN   Write LSN  Flush LSN  Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot  Slot Restart LSN  Slot WAL Status  Slot Safe WAL Size
----       --------   ---------  ---------  ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------  ----------------  ---------------  ------------------
sandbox-2  0/6053720  0/6053720  0/6053720  0/6053720   00:00:00   00:00:00   00:00:00    streaming  async       0              active            0/6053720         reserved         NULL
sandbox-3  0/6053720  0/6053720  0/6053720  0/6053720   00:00:00   00:00:00   00:00:00    streaming  async       0              active            0/6053720         reserved         NULL

Unmanaged Replication Slot Status
No unmanaged replication slots found

Managed roles status
No roles managed

Tablespaces status
No managed tablespaces

Pod Disruption Budgets status
Name             Role     Expected Pods  Current Healthy  Minimum Desired Healthy  Disruptions Allowed
----             ----     -------------  ---------------  -----------------------  -------------------
sandbox          replica  2              2                1                        1
sandbox-primary  primary  1              1                1                        0

Instances status
Name       Current LSN  Replication role  Status  QoS         Manager Version  Node
----       -----------  ----------------  ------  ---         ---------------  ----
sandbox-1  0/6053720    Primary           OK      BestEffort  1.24.3           k8s-eu-worker
sandbox-2  0/6053720    Standby (async)   OK      BestEffort  1.24.3           k8s-eu-worker2
sandbox-3  0/6053720    Standby (async)   OK      BestEffort  1.24.3           k8s-eu-worker
</code></pre>
<p>With an additional <code>-v</code> (e.g. <code>kubectl cnpg status sandbox -v -v</code>), you can
also view PostgreSQL configuration, HBA settings, and certificates.</p>
<p>The command also supports output in <code>yaml</code> and <code>json</code> format.</p>
<h3 id="promote">Promote</h3>
<p>The meaning of this command is to <code>promote</code> a pod in the cluster to primary, so you
can start with maintenance work or test a switch-over situation in your cluster:</p>
<pre><code class="language-sh">kubectl cnpg promote CLUSTER CLUSTER-INSTANCE
</code></pre>
<p>Or you can use the instance node number to promote:</p>
<pre><code class="language-sh">kubectl cnpg promote CLUSTER INSTANCE
</code></pre>
<h3 id="certificates">Certificates</h3>
<p>Clusters created using the CloudNativePG operator work with a CA to sign
a TLS authentication certificate.</p>
<p>To get a certificate, you need to provide a name for the secret to store
the credentials, the cluster name, and a user for this certificate:</p>
<pre><code class="language-sh">kubectl cnpg certificate cluster-cert --cnpg-cluster CLUSTER --cnpg-user USER
</code></pre>
<p>After the secret it's created, you can get it using <code>kubectl</code>:</p>
<pre><code class="language-sh">kubectl get secret cluster-cert
</code></pre>
<p>And the content of the same in plain text using the following commands:</p>
<pre><code class="language-sh">kubectl get secret cluster-cert -o json | jq -r '.data | map(@base64d) | .[]'
</code></pre>
<h3 id="restart">Restart</h3>
<p>The <code>kubectl cnpg restart</code> command can be used in two cases:</p>
<ul>
<li>
<p>requesting the operator to orchestrate a rollout restart
  for a certain cluster. This is useful to apply
  configuration changes to cluster dependent objects, such as <code>ConfigMaps</code>
  containing custom monitoring queries.</p>
</li>
<li>
<p>request a single instance restart, either in-place if the instance is
  the cluster's primary or deleting and recreating the pod if
  it is a replica.</p>
</li>
</ul>
<pre><code class="language-sh"># this command will restart a whole cluster in a rollout fashion
kubectl cnpg restart CLUSTER

# this command will restart a single instance, according to the policy above
kubectl cnpg restart CLUSTER INSTANCE
</code></pre>
<p>If the in-place restart is requested but the change cannot be applied without
a switchover, the switchover will take precedence over the in-place restart. A
common case for this will be a minor upgrade of PostgreSQL image.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want ConfigMaps and Secrets to be <strong>automatically</strong> reloaded
by instances, you can add a label with key <code>cnpg.io/reload</code> to it.</p>
</div>
<h3 id="reload">Reload</h3>
<p>The <code>kubectl cnpg reload</code> command requests the operator to trigger a reconciliation
loop for a certain cluster. This is useful to apply configuration changes
to cluster dependent objects, such as ConfigMaps containing custom monitoring queries.</p>
<p>The following command will reload all configurations for a given cluster:</p>
<pre><code class="language-sh">kubectl cnpg reload CLUSTER
</code></pre>
<h3 id="maintenance">Maintenance</h3>
<p>The <code>kubectl cnpg maintenance</code> command helps to modify one or more clusters
across namespaces and set the maintenance window values, it will change
the following fields:</p>
<ul>
<li>.spec.nodeMaintenanceWindow.inProgress</li>
<li>.spec.nodeMaintenanceWindow.reusePVC</li>
</ul>
<p>Accepts as argument <code>set</code> and <code>unset</code> using this to set the
<code>inProgress</code> to <code>true</code> in case <code>set</code>and to <code>false</code> in case of <code>unset</code>.</p>
<p>By default, <code>reusePVC</code> is always set to <code>false</code> unless the <code>--reusePVC</code> flag is passed.</p>
<p>The plugin will ask for a confirmation with a list of the cluster to modify
and their new values, if this is accepted this action will be applied to
all the cluster in the list.</p>
<p>If you want to set in maintenance all the PostgreSQL in your Kubernetes cluster,
just need to write the following command:</p>
<pre><code class="language-sh">kubectl cnpg maintenance set --all-namespaces
</code></pre>
<p>And you'll have the list of all the cluster to update</p>
<pre><code class="language-output">The following are the new values for the clusters
Namespace  Cluster Name     Maintenance  reusePVC
---------  ------------     -----------  --------
default    cluster-example  true         false
default    pg-backup        true         false
test       cluster-example  true         false
Do you want to proceed? [y/n]: y
</code></pre>
<h3 id="report">Report</h3>
<p>The <code>kubectl cnpg report</code> command bundles various pieces
of information into a ZIP file.
It aims to provide the needed context to debug problems
with clusters in production.</p>
<p>It has two sub-commands: <code>operator</code> and <code>cluster</code>.</p>
<h4 id="report-operator">report Operator</h4>
<p>The <code>operator</code> sub-command requests the operator to provide information
regarding the operator deployment, configuration and events.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>All confidential information in Secrets and ConfigMaps is REDACTED.
The Data map will show the <strong>keys</strong> but the values will be empty.
The flag <code>-S</code> / <code>--stopRedaction</code> will defeat the redaction and show the
values. Use only at your own risk, this will share private data.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By default, operator logs are not collected, but you can enable operator
log collection with the <code>--logs</code> flag</p>
</div>
<ul>
<li><strong>deployment information</strong>: the operator Deployment and operator Pod</li>
<li><strong>configuration</strong>: the Secrets and ConfigMaps in the operator namespace</li>
<li><strong>events</strong>: the Events in the operator namespace</li>
<li><strong>webhook configuration</strong>: the mutating and validating webhook configurations</li>
<li><strong>webhook service</strong>: the webhook service</li>
<li><strong>logs</strong>: logs for the operator Pod (optional, off by default) in JSON-lines format</li>
</ul>
<p>The command will generate a ZIP file containing various manifest in YAML format
(by default, but settable to JSON with the <code>-o</code> flag).
Use the <code>-f</code> flag to name a result file explicitly. If the <code>-f</code> flag is not used, a
default time-stamped filename is created for the zip file.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The report plugin obeys <code>kubectl</code> conventions, and will look for objects constrained
by namespace. The CNPG Operator will generally not be installed in the same
namespace as the clusters.
E.g. the default installation namespace is cnpg-system</p>
</div>
<pre><code class="language-sh">kubectl cnpg report operator -n cnpg-system
</code></pre>
<p>results in</p>
<pre><code class="language-output">Successfully written report to &quot;report_operator_&lt;TIMESTAMP&gt;.zip&quot; (format: &quot;yaml&quot;)
</code></pre>
<p>With the <code>-f</code> flag set:</p>
<pre><code class="language-sh">kubectl cnpg report operator -n cnpg-system -f reportRedacted.zip
</code></pre>
<p>Unzipping the file will produce a time-stamped top-level folder to keep the
directory tidy:</p>
<pre><code class="language-sh">unzip reportRedacted.zip
</code></pre>
<p>will result in:</p>
<pre><code class="language-output">Archive:  reportRedacted.zip
   creating: report_operator_&lt;TIMESTAMP&gt;/
   creating: report_operator_&lt;TIMESTAMP&gt;/manifests/
  inflating: report_operator_&lt;TIMESTAMP&gt;/manifests/deployment.yaml
  inflating: report_operator_&lt;TIMESTAMP&gt;/manifests/operator-pod.yaml
  inflating: report_operator_&lt;TIMESTAMP&gt;/manifests/events.yaml
  inflating: report_operator_&lt;TIMESTAMP&gt;/manifests/validating-webhook-configuration.yaml
  inflating: report_operator_&lt;TIMESTAMP&gt;/manifests/mutating-webhook-configuration.yaml
  inflating: report_operator_&lt;TIMESTAMP&gt;/manifests/webhook-service.yaml
  inflating: report_operator_&lt;TIMESTAMP&gt;/manifests/cnpg-ca-secret(secret).yaml
  inflating: report_operator_&lt;TIMESTAMP&gt;/manifests/cnpg-webhook-cert(secret).yaml
</code></pre>
<p>If you activated the <code>--logs</code> option, you'd see an extra subdirectory:</p>
<pre><code class="language-output">Archive:  report_operator_&lt;TIMESTAMP&gt;.zip
  &lt;snipped …&gt;
  creating: report_operator_&lt;TIMESTAMP&gt;/operator-logs/
  inflating: report_operator_&lt;TIMESTAMP&gt;/operator-logs/cnpg-controller-manager-66fb98dbc5-pxkmh-logs.jsonl
</code></pre>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The plugin will try to get the PREVIOUS operator's logs, which is helpful
when investigating restarted operators.
In all cases, it will also try to get the CURRENT operator logs. If current
and previous logs are available, it will show them both.</p>
</div>
<pre><code class="language-output">====== Begin of Previous Log =====
2023-03-28T12:56:41.251711811Z {&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:&quot;2023-03-28T12:56:41Z&quot;,&quot;logger&quot;:&quot;setup&quot;,&quot;msg&quot;:&quot;Starting CloudNativePG Operator&quot;,&quot;version&quot;:&quot;1.24.3&quot;,&quot;build&quot;:{&quot;Version&quot;:&quot;1.24.3+dev107&quot;,&quot;Commit&quot;:&quot;cc9bab17&quot;,&quot;Date&quot;:&quot;2023-03-28&quot;}}
2023-03-28T12:56:41.251851909Z {&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:&quot;2023-03-28T12:56:41Z&quot;,&quot;logger&quot;:&quot;setup&quot;,&quot;msg&quot;:&quot;Starting pprof HTTP server&quot;,&quot;addr&quot;:&quot;0.0.0.0:6060&quot;}
  &lt;snipped …&gt;

====== End of Previous Log =====
2023-03-28T12:57:09.854306024Z {&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:&quot;2023-03-28T12:57:09Z&quot;,&quot;logger&quot;:&quot;setup&quot;,&quot;msg&quot;:&quot;Starting CloudNativePG Operator&quot;,&quot;version&quot;:&quot;1.24.3&quot;,&quot;build&quot;:{&quot;Version&quot;:&quot;1.24.3+dev107&quot;,&quot;Commit&quot;:&quot;cc9bab17&quot;,&quot;Date&quot;:&quot;2023-03-28&quot;}}
2023-03-28T12:57:09.854363943Z {&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:&quot;2023-03-28T12:57:09Z&quot;,&quot;logger&quot;:&quot;setup&quot;,&quot;msg&quot;:&quot;Starting pprof HTTP server&quot;,&quot;addr&quot;:&quot;0.0.0.0:6060&quot;}
</code></pre>
<p>If the operator hasn't been restarted, you'll still see the <code>====== Begin …</code>
and  <code>====== End …</code> guards, with no content inside.</p>
<p>You can verify that the confidential information is REDACTED by default:</p>
<pre><code class="language-sh">cd report_operator_&lt;TIMESTAMP&gt;/manifests/
head cnpg-ca-secret\(secret\).yaml
</code></pre>
<pre><code class="language-yaml">data:
  ca.crt: &quot;&quot;
  ca.key: &quot;&quot;
metadata:
  creationTimestamp: &quot;2022-03-22T10:42:28Z&quot;
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
</code></pre>
<p>With the <code>-S</code> (<code>--stopRedaction</code>) option activated, secrets are shown:</p>
<pre><code class="language-sh">kubectl cnpg report operator -n cnpg-system -f reportNonRedacted.zip -S
</code></pre>
<p>You'll get a reminder that you're about to view confidential information:</p>
<pre><code class="language-output">WARNING: secret Redaction is OFF. Use it with caution
Successfully written report to &quot;reportNonRedacted.zip&quot; (format: &quot;yaml&quot;)
</code></pre>
<pre><code class="language-sh">unzip reportNonRedacted.zip
head cnpg-ca-secret\(secret\).yaml
</code></pre>
<pre><code class="language-yaml">data:
  ca.crt: LS0tLS1CRUdJTiBD…
  ca.key: LS0tLS1CRUdJTiBF…
metadata:
  creationTimestamp: &quot;2022-03-22T10:42:28Z&quot;
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
</code></pre>
<h4 id="report-cluster">report Cluster</h4>
<p>The <code>cluster</code> sub-command gathers the following:</p>
<ul>
<li><strong>cluster resources</strong>: the cluster information, same as <code>kubectl get cluster -o yaml</code></li>
<li><strong>cluster pods</strong>: pods in the cluster namespace matching the cluster name</li>
<li><strong>cluster jobs</strong>: jobs, if any, in the cluster namespace matching the cluster name</li>
<li><strong>events</strong>: events in the cluster namespace</li>
<li><strong>pod logs</strong>: logs for the cluster Pods (optional, off by default) in JSON-lines format</li>
<li><strong>job logs</strong>: logs for the Pods created by jobs (optional, off by default) in JSON-lines format</li>
</ul>
<p>The <code>cluster</code> sub-command accepts the <code>-f</code> and <code>-o</code> flags, as the <code>operator</code> does.
If the <code>-f</code> flag is not used, a default timestamped report name will be used.
Note that the cluster information does not contain configuration Secrets / ConfigMaps,
so the <code>-S</code> is disabled.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By default, cluster logs are not collected, but you can enable cluster
log collection with the <code>--logs</code> flag</p>
</div>
<p>Usage:</p>
<pre><code class="language-sh">kubectl cnpg report cluster CLUSTER [flags]
</code></pre>
<p>Note that, unlike the <code>operator</code> sub-command, for the <code>cluster</code> sub-command you
need to provide the cluster name, and very likely the namespace, unless the cluster
is in the default one.</p>
<pre><code class="language-sh">kubectl cnpg report cluster CLUSTER -f report.zip [-n NAMESPACE]
</code></pre>
<p>and then:</p>
<pre><code class="language-sh">unzip report.zip
</code></pre>
<pre><code class="language-output">Archive:  report.zip
   creating: report_cluster_example_&lt;TIMESTAMP&gt;/
   creating: report_cluster_example_&lt;TIMESTAMP&gt;/manifests/
  inflating: report_cluster_example_&lt;TIMESTAMP&gt;/manifests/cluster.yaml
  inflating: report_cluster_example_&lt;TIMESTAMP&gt;/manifests/cluster-pods.yaml
  inflating: report_cluster_example_&lt;TIMESTAMP&gt;/manifests/cluster-jobs.yaml
  inflating: report_cluster_example_&lt;TIMESTAMP&gt;/manifests/events.yaml
</code></pre>
<p>Remember that you can use the <code>--logs</code> flag to add the pod and job logs to the ZIP.</p>
<pre><code class="language-sh">kubectl cnpg report cluster CLUSTER [-n NAMESPACE] --logs
</code></pre>
<p>will result in:</p>
<pre><code class="language-output">Successfully written report to &quot;report_cluster_example_&lt;TIMESTAMP&gt;.zip&quot; (format: &quot;yaml&quot;)
</code></pre>
<pre><code class="language-sh">unzip report_cluster_&lt;TIMESTAMP&gt;.zip
</code></pre>
<pre><code class="language-output">Archive:  report_cluster_example_&lt;TIMESTAMP&gt;.zip
   creating: report_cluster_example_&lt;TIMESTAMP&gt;/
   creating: report_cluster_example_&lt;TIMESTAMP&gt;/manifests/
  inflating: report_cluster_example_&lt;TIMESTAMP&gt;/manifests/cluster.yaml
  inflating: report_cluster_example_&lt;TIMESTAMP&gt;/manifests/cluster-pods.yaml
  inflating: report_cluster_example_&lt;TIMESTAMP&gt;/manifests/cluster-jobs.yaml
  inflating: report_cluster_example_&lt;TIMESTAMP&gt;/manifests/events.yaml
   creating: report_cluster_example_&lt;TIMESTAMP&gt;/logs/
  inflating: report_cluster_example_&lt;TIMESTAMP&gt;/logs/cluster-example-full-1.jsonl
   creating: report_cluster_example_&lt;TIMESTAMP&gt;/job-logs/
  inflating: report_cluster_example_&lt;TIMESTAMP&gt;/job-logs/cluster-example-full-1-initdb-qnnvw.jsonl
  inflating: report_cluster_example_&lt;TIMESTAMP&gt;/job-logs/cluster-example-full-2-join-tvj8r.jsonl
</code></pre>
<h3 id="logs">Logs</h3>
<p>The <code>kubectl cnpg logs</code> command allows to follow the logs of a collection
of pods related to CloudNativePG in a single go.</p>
<p>It has at the moment one available sub-command: <code>cluster</code>.</p>
<h4 id="cluster-logs">Cluster logs</h4>
<p>The <code>cluster</code> sub-command gathers all the pod logs for a cluster in a single
stream or file.
This means that you can get all the pod logs in a single terminal window, with a
single invocation of the command.</p>
<p>As in all the cnpg plugin sub-commands, you can get instructions and help with
the <code>-h</code> flag:</p>
<p><code>kubectl cnpg logs cluster -h</code></p>
<p>The <code>logs</code> command will display logs in  JSON-lines format, unless the
<code>--timestamps</code> flag is used, in which case, a human-readable timestamp will be
prepended to each line. In this case, lines will no longer be valid JSON,
and tools such as <code>jq</code> may not work as desired.</p>
<p>If the <code>logs cluster</code> sub-command is given the <code>-f</code> flag (aka <code>--follow</code>), it
will follow the cluster pod logs, and will also watch for any new pods created
in the cluster after the command has been invoked.
Any new pods found, including pods that have been restarted or re-created,
will also have their pods followed.
The logs will be displayed in the terminal's standard-out.
This command will only exit when the cluster has no more pods left, or when it
is interrupted by the user.</p>
<p>If <code>logs</code> is called without the <code>-f</code> option, it will read the logs from all
cluster pods until the time of invocation and display them in the terminal's
standard-out, then exit.
The <code>-o</code> or <code>--output</code> flag can be provided, to specify the name
of the file where the logs should be saved, instead of displaying over
standard-out.
The <code>--tail</code> flag can be used to specify how many log lines will be retrieved
from each pod in the cluster. By default, the <code>logs cluster</code> sub-command will
display all the logs from each pod in the cluster. If combined with the "follow"
flag <code>-f</code>, the number of logs specified by <code>--tail</code> will be retrieved until the
current time, and from then the new logs will be followed.</p>
<p>NOTE: unlike other <code>cnpg</code> plugin commands, the <code>-f</code> is used to denote "follow"
rather than specify a file. This keeps with the convention of <code>kubectl logs</code>,
which takes <code>-f</code> to mean the logs should be followed.</p>
<p>Usage:</p>
<pre><code class="language-sh">kubectl cnpg logs cluster CLUSTER [flags]
</code></pre>
<p>Using the <code>-f</code> option to follow:</p>
<pre><code class="language-sh">kubectl cnpg report cluster CLUSTER -f
</code></pre>
<p>Using <code>--tail</code> option to display 3 lines from each pod and the <code>-f</code> option
to follow:</p>
<pre><code class="language-sh">kubectl cnpg report cluster CLUSTER -f --tail 3
</code></pre>
<pre><code class="language-output">{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:&quot;2023-06-30T13:37:33Z&quot;,&quot;logger&quot;:&quot;postgres&quot;,&quot;msg&quot;:&quot;2023-06-30 13:37:33.142 UTC [26] LOG:  ending log output to stderr&quot;,&quot;source&quot;:&quot;/controller/log/postgres&quot;,&quot;logging_pod&quot;:&quot;cluster-example-3&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:&quot;2023-06-30T13:37:33Z&quot;,&quot;logger&quot;:&quot;postgres&quot;,&quot;msg&quot;:&quot;2023-06-30 13:37:33.142 UTC [26] HINT:  Future log output will go to log destination \&quot;csvlog\&quot;.&quot;,&quot;source&quot;:&quot;/controller/log/postgres&quot;,&quot;logging_pod&quot;:&quot;cluster-example-3&quot;}
…
…
</code></pre>
<p>With the <code>-o</code> option omitted, and with <code>--output</code> specified:</p>
<pre><code class="language-console">$ kubectl cnpg logs cluster CLUSTER --output my-cluster.log

Successfully written logs to &quot;my-cluster.log&quot;
</code></pre>
<h4 id="pretty">Pretty</h4>
<p>The <code>pretty</code> sub-command reads a log stream from standard input, formats it
into a human-readable output, and attempts to sort the entries by timestamp.</p>
<p>It can be used in combination with <code>kubectl cnpg logs cluster</code>, as
shown in the following example:</p>
<pre><code class="language-console">$ kubectl cnpg logs cluster cluster-example | kubectl cnpg logs pretty
2024-10-15T17:35:00.336 INFO     cluster-example-1 instance-manager Starting CloudNativePG Instance Manager
2024-10-15T17:35:00.336 INFO     cluster-example-1 instance-manager Checking for free disk space for WALs before starting PostgreSQL
2024-10-15T17:35:00.347 INFO     cluster-example-1 instance-manager starting tablespace manager
2024-10-15T17:35:00.347 INFO     cluster-example-1 instance-manager starting external server manager
[...]
</code></pre>
<p>Alternatively, it can be used in combination with other commands that produce
CNPG logs in JSON format, such as <code>stern</code>, or <code>kubectl logs</code>, as in the
following example:</p>
<pre><code class="language-console">$ kubectl logs cluster-example-1 | kubectl cnpg logs pretty
2024-10-15T17:35:00.336 INFO     cluster-example-1 instance-manager Starting CloudNativePG Instance Manager
2024-10-15T17:35:00.336 INFO     cluster-example-1 instance-manager Checking for free disk space for WALs before starting PostgreSQL
2024-10-15T17:35:00.347 INFO     cluster-example-1 instance-manager starting tablespace manager
2024-10-15T17:35:00.347 INFO     cluster-example-1 instance-manager starting external server manager
[...]
</code></pre>
<p>The <code>pretty</code> sub-command also supports advanced log filtering, allowing users
to display logs for specific pods or loggers, or to filter logs by severity
level.
Here's an example:</p>
<pre><code class="language-console">$ kubectl cnpg logs cluster cluster-example | kubectl cnpg logs pretty --pods cluster-example-1 --loggers postgres --log-level info
2024-10-15T17:35:00.509 INFO     cluster-example-1 postgres         2024-10-15 17:35:00.509 UTC [29] LOG:  redirecting log output to logging collector process
2024-10-15T17:35:00.509 INFO     cluster-example-1 postgres         2024-10-15 17:35:00.509 UTC [29] HINT:  Future log output will appear in directory &quot;/controller/log&quot;...
2024-10-15T17:35:00.510 INFO     cluster-example-1 postgres         2024-10-15 17:35:00.509 UTC [29] LOG:  ending log output to stderr
2024-10-15T17:35:00.510 INFO     cluster-example-1 postgres         ending log output to stderr
[...]
</code></pre>
<p>The <code>pretty</code> sub-command will try to sort the log stream,
to make logs easier to reason about. In order to achieve this, it gathers the
logs into groups, and within groups it sorts by timestamp. This is the only
way to sort interactively, as <code>pretty</code> may be piped from a command in "follow"
mode. The sub-command will add a group separator line, <code>---</code>, at the end of
each sorted group. The size of the grouping can be configured via the
<code>--sorting-group-size</code> flag (default: 1000), as illustrated in the following example:</p>
<pre><code class="language-console">$ kubectl cnpg logs cluster cluster-example | kubectl cnpg logs pretty --sorting-group-size=3
2024-10-15T17:35:20.426 INFO     cluster-example-2 instance-manager Starting CloudNativePG Instance Manager
2024-10-15T17:35:20.426 INFO     cluster-example-2 instance-manager Checking for free disk space for WALs before starting PostgreSQL
2024-10-15T17:35:20.438 INFO     cluster-example-2 instance-manager starting tablespace manager
---
2024-10-15T17:35:20.438 INFO     cluster-example-2 instance-manager starting external server manager
2024-10-15T17:35:20.438 INFO     cluster-example-2 instance-manager starting controller-runtime manager
2024-10-15T17:35:20.439 INFO     cluster-example-2 instance-manager Starting EventSource
---
[...]
</code></pre>
<p>To explore all available options, use the <code>-h</code> flag for detailed explanations
of the supported flags and their usage.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>You can also increase the verbosity of the log by adding more <code>-v</code> options.</p>
</div>
<h3 id="destroy">Destroy</h3>
<p>The <code>kubectl cnpg destroy</code> command helps remove an instance and all the
associated PVCs from a Kubernetes cluster.</p>
<p>The optional <code>--keep-pvc</code> flag, if specified, allows you to keep the PVCs,
while removing all <code>metadata.ownerReferences</code> that were set by the instance.
Additionally, the <code>cnpg.io/pvcStatus</code> label on the PVCs will change from
<code>ready</code> to <code>detached</code> to signify that they are no longer in use.</p>
<p>Running again the command without the <code>--keep-pvc</code> flag will remove the
detached PVCs.</p>
<p>Usage:</p>
<pre><code class="language-sh">kubectl cnpg destroy CLUSTER INSTANCE
</code></pre>
<p>The following example removes the <code>cluster-example-2</code> pod and the associated
PVCs:</p>
<pre><code class="language-sh">kubectl cnpg destroy cluster-example 2
</code></pre>
<h3 id="cluster-hibernation">Cluster hibernation</h3>
<p>Sometimes you may want to suspend the execution of a CloudNativePG <code>Cluster</code>
while retaining its data, then resume its activity at a later time. We've
called this feature <strong>cluster hibernation</strong>.</p>
<p>Hibernation is only available via the <code>kubectl cnpg hibernate [on|off]</code>
commands.</p>
<p>Hibernating a CloudNativePG cluster means destroying all the resources
generated by the cluster, except the PVCs that belong to the PostgreSQL primary
instance.</p>
<p>You can hibernate a cluster with:</p>
<pre><code class="language-sh">kubectl cnpg hibernate on CLUSTER
</code></pre>
<p>This will:</p>
<ol>
<li>shutdown every PostgreSQL instance</li>
<li>detach the PVCs containing the data of the primary instance, and annotate
   them with the latest database status and the latest cluster configuration</li>
<li>delete the <code>Cluster</code> resource, including every generated resource - except
   the aforementioned PVCs</li>
</ol>
<p>When hibernated, a CloudNativePG cluster is represented by just a group of
PVCs, in which the one containing the <code>PGDATA</code> is annotated with the latest
available status, including content from <code>pg_controldata</code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>A cluster having fenced instances cannot be hibernated, as fencing is
part of the hibernation procedure too.</p>
</div>
<p>In case of error the operator will not be able to revert the procedure. You can
still force the operation with:</p>
<pre><code class="language-sh">kubectl cnpg hibernate on CLUSTER --force
</code></pre>
<p>A hibernated cluster can be resumed with:</p>
<pre><code class="language-sh">kubectl cnpg hibernate off CLUSTER
</code></pre>
<p>Once the cluster has been hibernated, it's possible to show the last
configuration and the status that PostgreSQL had after it was shut down.
That can be done with:</p>
<pre><code class="language-sh">kubectl cnpg hibernate status CLUSTER
</code></pre>
<h3 id="benchmarking-the-database-with-pgbench">Benchmarking the database with pgbench</h3>
<p>Pgbench can be run against an existing PostgreSQL cluster with following
command:</p>
<pre><code class="language-sh">kubectl cnpg pgbench CLUSTER -- --time 30 --client 1 --jobs 1
</code></pre>
<p>Refer to the <a href="../benchmarking/#pgbench">Benchmarking pgbench section</a> for more
details.</p>
<h3 id="benchmarking-the-storage-with-fio">Benchmarking the storage with fio</h3>
<p><code>fio</code> can be run on an existing storage class with following command:</p>
<pre><code class="language-sh">kubectl cnpg fio FIO_JOB_NAME [-n NAMESPACE]
</code></pre>
<p>Refer to the <a href="../benchmarking/#fio">Benchmarking fio section</a> for more details.</p>
<h3 id="requesting-a-new-physical-backup">Requesting a new physical backup</h3>
<p>The <code>kubectl cnpg backup</code> command requests a new physical backup for
an existing Postgres cluster by creating a new <code>Backup</code> resource.</p>
<p>The following example requests an on-demand backup for a given cluster:</p>
<pre><code class="language-sh">kubectl cnpg backup CLUSTER
</code></pre>
<p>or, if using volume snapshots:</p>
<pre><code class="language-sh">kubectl cnpg backup CLUSTER -m volumeSnapshot
</code></pre>
<p>The created backup will be named after the request time:</p>
<pre><code class="language-console">$ kubectl cnpg backup cluster-example
backup/cluster-example-20230121002300 created
</code></pre>
<p>By default, a newly created backup will use the backup target policy defined
in the cluster to choose which instance to run on.
However, you can override this policy with the <code>--backup-target</code> option.</p>
<p>In the case of volume snapshot backups, you can also use the <code>--online</code> option
to request an online/hot backup or an offline/cold one: additionally, you can
also tune online backups by explicitly setting the <code>--immediate-checkpoint</code> and
<code>--wait-for-archive</code> options.</p>
<p>The <a href="../backup/#backup">"Backup" section</a> contains more information about
the configuration settings.</p>
<h3 id="launching-psql">Launching psql</h3>
<p>The <code>kubectl cnpg psql CLUSTER</code> command starts a new PostgreSQL interactive front-end
process (psql) connected to an existing Postgres cluster, as if you were running
it from the actual pod. This means that you will be using the <code>postgres</code> user.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>As you will be connecting as <code>postgres</code> user, in production environments this
method should be used with extreme care, by authorized personnel only.</p>
</div>
<pre><code class="language-console">$ kubectl cnpg psql cluster-example

psql (17.4 (Debian 17.4-1.pgdg110+1))
Type &quot;help&quot; for help.

postgres=#
</code></pre>
<p>By default, the command will connect to the primary instance. The user can
select to work against a replica by using the <code>--replica</code> option:</p>
<pre><code class="language-console">$ kubectl cnpg psql --replica cluster-example

psql (17.4 (Debian 17.4-1.pgdg110+1))

Type &quot;help&quot; for help.

postgres=# select pg_is_in_recovery();
 pg_is_in_recovery
-------------------
 t
(1 row)

postgres=# \q
</code></pre>
<p>This command will start <code>kubectl exec</code>, and the <code>kubectl</code> executable must be
reachable in your <code>PATH</code> variable to correctly work.</p>
<h3 id="snapshotting-a-postgres-cluster">Snapshotting a Postgres cluster</h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <code>kubectl cnpg snapshot</code> command has been removed.
Please use the <a href="#requesting-a-new-physical-backup"><code>backup</code> command</a> to request
backups using volume snapshots.</p>
</div>
<h3 id="using-pgadmin4-for-evaluationdemonstration-purposes-only">Using pgAdmin4 for evaluation/demonstration purposes only</h3>
<p><a href="https://www.pgadmin.org/">pgAdmin</a> stands as the most popular and feature-rich
open-source administration and development platform for PostgreSQL.
For more information on the project, please refer to the official
<a href="https://www.pgadmin.org/docs/">documentation</a>.</p>
<p>Given that the pgAdmin Development Team maintains official Docker container
images, you can install pgAdmin in your environment as a standard
Kubernetes deployment.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Deployment of pgAdmin in Kubernetes production environments is beyond the
scope of this document and, more broadly, of the CloudNativePG project.</p>
</div>
<p>However, <strong>for the purposes of demonstration and evaluation</strong>, CloudNativePG
offers a suitable solution. The <code>cnpg</code> plugin implements the <code>pgadmin4</code>
command, providing a straightforward method to connect to a given database
<code>Cluster</code> and navigate its content in a local environment such as <code>kind</code>.</p>
<p>For example, you can install a demo deployment of pgAdmin4 for the
<code>cluster-example</code> cluster as follows:</p>
<pre><code class="language-sh">kubectl cnpg pgadmin4 cluster-example
</code></pre>
<p>This command will produce:</p>
<pre><code class="language-output">ConfigMap/cluster-example-pgadmin4 created
Deployment/cluster-example-pgadmin4 created
Service/cluster-example-pgadmin4 created
Secret/cluster-example-pgadmin4 created

[...]
</code></pre>
<p>After deploying pgAdmin, forward the port using kubectl and connect
through your browser by following the on-screen instructions.</p>
<p><img alt="Screenshot of desktop installation of pgAdmin" src="../images/pgadmin4.png" /></p>
<p>As usual, you can use the <code>--dry-run</code> option to generate the YAML file:</p>
<pre><code class="language-sh">kubectl cnpg pgadmin4 --dry-run cluster-example
</code></pre>
<p>pgAdmin4 can be installed in either desktop or server mode, with the default
being server.</p>
<p>In <code>server</code> mode, authentication is required using a randomly generated password,
and users must manually specify the database to connect to.</p>
<p>On the other hand, <code>desktop</code> mode initiates a pgAdmin web interface without
requiring authentication. It automatically connects to the <code>app</code> database as the
<code>app</code> user, making it ideal for quick demos, such as on a local deployment using
<code>kind</code>:</p>
<pre><code class="language-sh">kubectl cnpg pgadmin4 --mode desktop cluster-example
</code></pre>
<p>After concluding your demo, ensure the termination of the pgAdmin deployment by
executing:</p>
<pre><code class="language-sh">kubectl cnpg pgadmin4 --dry-run cluster-example | kubectl delete -f -
</code></pre>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Never deploy pgAdmin in production using the plugin.</p>
</div>
<h3 id="logical-replication-publications">Logical Replication Publications</h3>
<p>The <code>cnpg publication</code> command group is designed to streamline the creation and
removal of <a href="https://www.postgresql.org/docs/current/logical-replication-publication.html">PostgreSQL logical replication publications</a>.
Be aware that these commands are primarily intended for assisting in the
creation of logical replication publications, particularly on remote PostgreSQL
databases.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>It is crucial to have a solid understanding of both the capabilities and
limitations of PostgreSQL's native logical replication system before using
these commands.
In particular, be mindful of the <a href="https://www.postgresql.org/docs/current/logical-replication-restrictions.html">logical replication restrictions</a>.</p>
</div>
<h4 id="creating-a-new-publication">Creating a new publication</h4>
<p>To create a logical replication publication, use the <code>cnpg publication create</code>
command. The basic structure of this command is as follows:</p>
<pre><code class="language-sh">kubectl cnpg publication create \
  --publication PUBLICATION_NAME \
  [--external-cluster EXTERNAL_CLUSTER]
  LOCAL_CLUSTER [options]
</code></pre>
<p>There are two primary use cases:</p>
<ul>
<li>
<p>With <code>--external-cluster</code>: Use this option to create a publication on an
  external cluster (i.e. defined in the <code>externalClusters</code> stanza). The commands
  will be issued from the <code>LOCAL_CLUSTER</code>, but the publication will be for the
  data in <code>EXTERNAL_CLUSTER</code>.</p>
</li>
<li>
<p>Without <code>--external-cluster</code>: Use this option to create a publication in the
  <code>LOCAL_CLUSTER</code> PostgreSQL <code>Cluster</code> (by default, the <code>app</code> database).</p>
</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When connecting to an external cluster, ensure that the specified user has
sufficient permissions to execute the <code>CREATE PUBLICATION</code> command.</p>
</div>
<p>You have several options, similar to the <a href="https://www.postgresql.org/docs/current/sql-createpublication.html"><code>CREATE PUBLICATION</code></a>
command, to define the group of tables to replicate. Notable options include:</p>
<ul>
<li>If you specify the <code>--all-tables</code> option, you create a publication <code>FOR ALL TABLES</code>.</li>
<li>Alternatively, you can specify multiple occurrences of:</li>
<li><code>--table</code>: Add a specific table (with an expression) to the publication.</li>
<li><code>--schema</code>: Include all tables in the specified database schema (available
    from PostgreSQL 15).</li>
</ul>
<p>The <code>--dry-run</code> option enables you to preview the SQL commands that the plugin
will execute.</p>
<p>For additional information and detailed instructions, type the following
command:</p>
<pre><code class="language-sh">kubectl cnpg publication create --help
</code></pre>
<h5 id="example">Example</h5>
<p>Given a <code>source-cluster</code> and a <code>destination-cluster</code>, we would like to create a
publication for the data on <code>source-cluster</code>.
The <code>destination-cluster</code> has an entry in the <code>externalClusters</code> stanza pointing
to <code>source-cluster</code>.</p>
<p>We can run:</p>
<pre><code class="language-sh">kubectl cnpg publication create destination-cluster  \
  --external-cluster=source-cluster --all-tables
</code></pre>
<p>which will create a publication for all tables on <code>source-cluster</code>, running
the SQL commands on the <code>destination-cluster</code>.</p>
<p>Or instead, we can run:</p>
<pre><code class="language-sh">kubectl cnpg publication create source-cluster \
  --publication=app --all-tables
</code></pre>
<p>which will create a publication named <code>app</code> for all the tables in the
<code>source-cluster</code>, running the SQL commands on the source cluster.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>There are two sample files that have been provided for illustration and inspiration:
<a href="../samples/cluster-example-logical-source.yaml">logical-source</a> and
<a href="../samples/cluster-example-logical-destination.yaml">logical-destination</a>.</p>
</div>
<h4 id="dropping-a-publication">Dropping a publication</h4>
<p>The <code>cnpg publication drop</code> command seamlessly complements the <code>create</code> command
by offering similar key options, including the publication name, cluster name,
and an optional external cluster. You can drop a <code>PUBLICATION</code> with the
following command structure:</p>
<pre><code class="language-sh">kubectl cnpg publication drop \
  --publication PUBLICATION_NAME \
  [--external-cluster EXTERNAL_CLUSTER]
  LOCAL_CLUSTER [options]
</code></pre>
<p>To access further details and precise instructions, use the following command:</p>
<pre><code class="language-sh">kubectl cnpg publication drop --help
</code></pre>
<h3 id="logical-replication-subscriptions">Logical Replication Subscriptions</h3>
<p>The <code>cnpg subscription</code> command group is a dedicated set of commands designed
to simplify the creation and removal of
<a href="https://www.postgresql.org/docs/current/logical-replication-subscription.html">PostgreSQL logical replication subscriptions</a>.
These commands are specifically crafted to aid in the establishment of logical
replication subscriptions, especially when dealing with remote PostgreSQL
databases.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Before using these commands, it is essential to have a comprehensive
understanding of both the capabilities and limitations of PostgreSQL's
native logical replication system.
In particular, be mindful of the <a href="https://www.postgresql.org/docs/current/logical-replication-restrictions.html">logical replication restrictions</a>.</p>
</div>
<p>In addition to subscription management, we provide a helpful command for
synchronizing all sequences from the source cluster. While its applicability
may vary, this command can be particularly useful in scenarios involving major
upgrades or data import from remote servers.</p>
<h4 id="creating-a-new-subscription">Creating a new subscription</h4>
<p>To create a logical replication subscription, use the <code>cnpg subscription create</code>
command. The basic structure of this command is as follows:</p>
<pre><code class="language-sh">kubectl cnpg subscription create \
  --subscription SUBSCRIPTION_NAME \
  --publication PUBLICATION_NAME \
  --external-cluster EXTERNAL_CLUSTER \
  LOCAL_CLUSTER [options]
</code></pre>
<p>This command configures a subscription directed towards the specified
publication in the designated external cluster, as defined in the
<code>externalClusters</code> stanza of the <code>LOCAL_CLUSTER</code>.</p>
<p>For additional information and detailed instructions, type the following
command:</p>
<pre><code class="language-sh">kubectl cnpg subscription create --help
</code></pre>
<h5 id="example_1">Example</h5>
<p>As in the section on publications, we have a <code>source-cluster</code> and a
<code>destination-cluster</code>, and we have already created a publication called
<code>app</code>.</p>
<p>The following command:</p>
<pre><code class="language-sh">kubectl cnpg subscription create destination-cluster \
  --external-cluster=source-cluster \
  --publication=app --subscription=app
</code></pre>
<p>will create a subscription for <code>app</code> on the destination cluster.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Prioritize testing subscriptions in a non-production environment to ensure
their effectiveness and identify any potential issues before implementing them
in a production setting.</p>
</div>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>There are two sample files that have been provided for illustration and inspiration:
<a href="../samples/cluster-example-logical-source.yaml">logical-source</a> and
<a href="../samples/cluster-example-logical-destination.yaml">logical-destination</a>.</p>
</div>
<h4 id="dropping-a-subscription">Dropping a subscription</h4>
<p>The <code>cnpg subscription drop</code> command seamlessly complements the <code>create</code> command.
You can drop a <code>SUBSCRIPTION</code> with the following command structure:</p>
<pre><code class="language-sh">kubectl cnpg subcription drop \
  --subscription SUBSCRIPTION_NAME \
  LOCAL_CLUSTER [options]
</code></pre>
<p>To access further details and precise instructions, use the following command:</p>
<pre><code class="language-sh">kubectl cnpg subscription drop --help
</code></pre>
<h4 id="synchronizing-sequences">Synchronizing sequences</h4>
<p>One notable constraint of PostgreSQL logical replication, implemented through
publications and subscriptions, is the lack of sequence synchronization. This
becomes particularly relevant when utilizing logical replication for live
database migration, especially to a higher version of PostgreSQL. A crucial
step in this process involves updating sequences before transitioning
applications to the new database (<em>cutover</em>).</p>
<p>To address this limitation, the <code>cnpg subscription sync-sequences</code> command
offers a solution. This command establishes a connection with the source
database, retrieves all relevant sequences, and subsequently updates local
sequences with matching identities (based on database schema and sequence
name).</p>
<p>You can use the command as shown below:</p>
<pre><code class="language-sh">kubectl cnpg subscription sync-sequences \
  --subscription SUBSCRIPTION_NAME \
  LOCAL_CLUSTER
</code></pre>
<p>For comprehensive details and specific instructions, utilize the following
command:</p>
<pre><code class="language-sh">kubectl cnpg subscription sync-sequences --help
</code></pre>
<h5 id="example_2">Example</h5>
<p>As in the previous sections for publication and subscription, we have
a <code>source-cluster</code> and a <code>destination-cluster</code>. The publication and the
subscription, both called <code>app</code>, are already present.</p>
<p>The following command will synchronize the sequences involved in the
<code>app</code> subscription, from the source cluster into the destination cluster.</p>
<pre><code class="language-sh">kubectl cnpg subscription sync-sequences destination-cluster \
  --subscription=app
</code></pre>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Prioritize testing subscriptions in a non-production environment to
guarantee their effectiveness and detect any potential issues before deploying
them in a production setting.</p>
</div>
<h2 id="integration-with-k9s">Integration with K9s</h2>
<p>The <code>cnpg</code> plugin can be easily integrated in <a href="https://k9scli.io/">K9s</a>, a
popular terminal-based UI to interact with Kubernetes clusters.</p>
<p>See <a href="../samples/k9s/plugins.yml"><code>k9s/plugins.yml</code></a> for details.</p>
<h2 id="permissions-required-by-the-plugin">Permissions required by the plugin</h2>
<p>The plugin requires a set of Kubernetes permissions that depends on the command
to execute. These permissions may affect resources and sub-resources like Pods,
PDBs, PVCs, and enable actions like <code>get</code>, <code>delete</code>, <code>patch</code>. The following
table contains the full details:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Command</th>
<th style="text-align: left;">Resource Permissions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">backup</td>
<td style="text-align: left;">clusters: get<br/>backups: create</td>
</tr>
<tr>
<td style="text-align: left;">certificate</td>
<td style="text-align: left;">clusters: get<br/>secrets: get,create</td>
</tr>
<tr>
<td style="text-align: left;">destroy</td>
<td style="text-align: left;">pods: get,delete<br/>jobs: delete,list<br/>PVCs: list,delete,update</td>
</tr>
<tr>
<td style="text-align: left;">fencing</td>
<td style="text-align: left;">clusters: get,patch<br/>pods: get</td>
</tr>
<tr>
<td style="text-align: left;">fio</td>
<td style="text-align: left;">PVCs: create<br/>configmaps: create<br/>deployment: create</td>
</tr>
<tr>
<td style="text-align: left;">hibernate</td>
<td style="text-align: left;">clusters: get,patch,delete<br/>pods: list,get,delete<br/>pods/exec: create<br/>jobs: list<br/>PVCs: get,list,update,patch,delete</td>
</tr>
<tr>
<td style="text-align: left;">install</td>
<td style="text-align: left;">none</td>
</tr>
<tr>
<td style="text-align: left;">logs</td>
<td style="text-align: left;">clusters: get<br/>pods: list<br/>pods/log: get</td>
</tr>
<tr>
<td style="text-align: left;">maintenance</td>
<td style="text-align: left;">clusters: get,patch,list<br/></td>
</tr>
<tr>
<td style="text-align: left;">pgadmin4</td>
<td style="text-align: left;">clusters: get<br/>configmaps: create<br/>deployments: create<br/>services: create<br/>secrets: create</td>
</tr>
<tr>
<td style="text-align: left;">pgbench</td>
<td style="text-align: left;">clusters: get<br/>jobs: create<br/></td>
</tr>
<tr>
<td style="text-align: left;">promote</td>
<td style="text-align: left;">clusters: get<br/>clusters/status: patch<br/>pods: get</td>
</tr>
<tr>
<td style="text-align: left;">psql</td>
<td style="text-align: left;">pods: get,list<br/>pods/exec: create</td>
</tr>
<tr>
<td style="text-align: left;">publication</td>
<td style="text-align: left;">clusters: get<br/>pods: get,list<br/>pods/exec: create</td>
</tr>
<tr>
<td style="text-align: left;">reload</td>
<td style="text-align: left;">clusters: get,patch</td>
</tr>
<tr>
<td style="text-align: left;">report cluster</td>
<td style="text-align: left;">clusters: get<br/>pods: list<br/>pods/log: get<br/>jobs: list<br/>events: list<br/>PVCs: list</td>
</tr>
<tr>
<td style="text-align: left;">report operator</td>
<td style="text-align: left;">configmaps: get<br/>deployments: get<br/>events: list<br/>pods: list<br/>pods/log: get<br/>secrets: get<br/>services: get<br/>mutatingwebhookconfigurations: list<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup><br/> validatingwebhookconfigurations: list<sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">1</a></sup><br/> If OLM is present on the K8s cluster, also:<br/>clusterserviceversions: list<br/>installplans: list<br/>subscriptions: list</td>
</tr>
<tr>
<td style="text-align: left;">restart</td>
<td style="text-align: left;">clusters: get,patch<br/>pods: get,delete</td>
</tr>
<tr>
<td style="text-align: left;">status</td>
<td style="text-align: left;">clusters: get<br/>pods: list<br/>pods/exec: create<br/>pods/proxy: create<br/>PDBs: list</td>
</tr>
<tr>
<td style="text-align: left;">subscription</td>
<td style="text-align: left;">clusters: get<br/>pods: get,list<br/>pods/exec: create</td>
</tr>
<tr>
<td style="text-align: left;">version</td>
<td style="text-align: left;">none</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>The permissions are cluster scope ClusterRole resources.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
<p>Additionally, assigning the <code>list</code> permission on the <code>clusters</code> will enable
autocompletion for multiple commands.</p>
<h3 id="role-examples">Role examples</h3>
<p>It is possible to create roles with restricted permissions.
The following example creates a role that only has access to the cluster logs:</p>
<pre><code class="language-yaml">---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cnpg-log
rules:
  - verbs:
      - get
    apiGroups:
      - postgresql.cnpg.io
    resources:
      - clusters
  - verbs:
      - list
    apiGroups:
      - ''
    resources:
      - pods
  - verbs:
      - get
    apiGroups:
      - ''
    resources:
      - pods/log
</code></pre>
<p>The next example shows a role with the minimal permissions required to get
the cluster status using the plugin's <code>status</code> command:</p>
<pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cnpg-status
rules:
  - verbs:
      - get
    apiGroups:
      - postgresql.cnpg.io
    resources:
      - clusters
  - verbs:
      - list
    apiGroups:
      - ''
    resources:
      - pods
  - verbs:
      - create
    apiGroups:
      - ''
    resources:
      - pods/exec
  - verbs:
      - create
    apiGroups:
      - ''
    resources:
      - pods/proxy
  - verbs:
      - list
    apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
</code></pre>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Keeping the verbs restricted per <code>resources</code> and per <code>apiGroups</code> helps to
prevent inadvertently granting more than intended permissions.</p>
</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../kubernetes_upgrade/" class="btn btn-neutral float-left" title="Kubernetes Upgrade and Maintenance"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../failover/" class="btn btn-neutral float-right" title="Automated failover">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../kubernetes_upgrade/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../failover/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
